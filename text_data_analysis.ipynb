{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dany-xu/AI-Generated-Text-Detection-using-LLM/blob/main/text_data_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Xi6jhXTDJ6C",
        "outputId": "c7f9d947-a6bf-4cbd-db86-f3391fe1fa48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=523a253c5e7e3145687d399f12a2d20d3934c84e30cd6635262752df472e5590\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poFe5LpcCv1u",
        "outputId": "4e58f30d-73fc-4a15-88a0-41ed5a20d728"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Insert Data"
      ],
      "metadata": {
        "id": "or8_0_N9EL7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filepath = \"./drive/My Drive/Big Data Project/Data/\"\n",
        "\n",
        "# Retreive the xlsx format data\n",
        "# Convert data into dataframe format\n",
        "try:\n",
        "  fusion_df = pd.read_excel(filepath + \"ieee-chatgpt-fusion.xlsx\").drop(columns=[\"Unnamed: 0\",\"index\"])\n",
        "  generation_df = pd.read_excel(filepath + \"ieee-chatgpt-generation.xlsx\").drop(columns=[\"Unnamed: 0\",\"Unnamed: 0.1\"])\n",
        "  polish_df = pd.read_excel(filepath + \"ieee-chatgpt-polish.xlsx\")\n",
        "  human_df = pd.read_excel(filepath + \"ieee-init.xlsx\")\n",
        "  print(\"Successfully insert data to Colab!\")\n",
        "\n",
        "except Exception as e:\n",
        "  print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bc4k1jfZC4bq",
        "outputId": "0020a304-cc15-4ae7-d3ea-4ac834b430d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully insert data to Colab!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"AIGeneratedText\").getOrCreate()"
      ],
      "metadata": {
        "id": "6Y0fPfu8DI2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Conversion\n",
        "Convert Data to Spark DataFrame"
      ],
      "metadata": {
        "id": "2bD-t4TtEP9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert data into Spark DataFrame format\n",
        "try:\n",
        "  fusion = spark.createDataFrame(fusion_df)\n",
        "  generation = spark.createDataFrame(generation_df)\n",
        "  polish = spark.createDataFrame(polish_df)\n",
        "  human = spark.createDataFrame(human_df)\n",
        "  print(\"Successfully convert data into Spark DataFrame\")\n",
        "except Exception as e:\n",
        "  print(f\"Error occured: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzD9BEcVD8ky",
        "outputId": "f4994d85-2081-4109-c1be-8ec4fda413b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully convert data into Spark DataFrame\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Analysis"
      ],
      "metadata": {
        "id": "ey1nhhV9EsnF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fusion**"
      ],
      "metadata": {
        "id": "8gjxHkdZEvsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the structure of the table\n",
        "fusion.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coQshBxkD-Pw",
        "outputId": "caaa1b3f-ecac-4c4f-ac32-f32d8b60b9a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- title: string (nullable = true)\n",
            " |-- keyword: string (nullable = true)\n",
            " |-- abstract: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Showcase the top five rows data\n",
        "fusion.show(10, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35ZNaF5ZGH_x",
        "outputId": "8b729588-d3e8-4f5b-cb4c-5a0e7e061c82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|id     |title                                                                                                           |keyword                                                                                                                                   |abstract                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
            "+-------+----------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|8600003|An Improved Variable-Node-Based BP Decoding Algorithm for NAND Flash Memory                                     |\"Flash memories\",\"Reliability\",\"Decoding\",\"Parity check codes\",\"Convergence\",\"Error probability\",\"Threshold voltage\"                      |To solve the problems of the data reliability for NAND flash storages, a variable-node-based belief-propagation with message pre-processing (VNBP-MP) decoding algorithm for binary low-density parity-check (LDPC) codes is proposed. The major feature is that, by making use of the characteristics of the NAND flash channel, the proposed algorithm performs the message pre-processing (MP) scheme to effectively prevent the propagation of unreliable messages and speed up the propagation of reliable messages. Additionally, the VNBP-MP algorithm includes a treatment for oscillating variable nodes (VNs) to further accelerate decoding convergence. Simulation results show that the proposed VNBP-MP algorithm has a noticeable improvement in convergence speed without compromising the error-correction performance, compared with the existing algorithms.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
            "|8600004|Mobile Robot Location Algorithm Based on Improved Particle Filtering                                            |\"Sociology\",\"Statistics\",\"Simultaneous localization and mapping\",\"Genetic algorithms\",\"Estimation\",\"Filtering\"                            |Many techniques have been proposed to solve the simultaneous localization and mapping (SLAM) problem, and among them, the Particle Filter (PF) is considered to be one of the most effective ways. However, the PF algorithm needs a large number of samples to approximate the posterior probability density of the system, which makes the algorithm complex. Furthermore, the judgment of resampling is imperfect. Based on this, an improved PF algorithm which introducing population diversity factor and genetic algorithm into the process of re-sampling is proposed in this paper. The effective sample size and the population diversity factor are used to determine whether to re-sampling. When re-sampling is needed, the genetic algorithm is used to optimize the particle set. The simulation result shows that estimation accuracy of the improved algorithm is better than that of traditional particles filter, not only in accuracy, but also in efficiency.                                                                                                                                                                                                                                                                                                                                                                                                |\n",
            "|8600008|Vertical Handoff Decision Algorithm for Heterogeneous Wireless Networks Based on Entropy and Improved TOPSIS    |\"Entropy\",\"Handover\",\"Wireless networks\",\"Decision making\",\"Heterogeneous networks\"                                                       |In the future scenario of multiple wireless network coverage, the choice of vertical handoff decision algorithm will directly affect the continuity of the session, the mobility of the user, and seamless roaming under heterogeneous wireless networks. Therefore, the study of vertical handover related algorithms is the key to the success of various wireless access networks in the future. In this regard, this paper proposes an optimized algorithm that combines two multiple attribute decision making (MADM) techniques, namely the Entropy and the improved Technique for Order Preference by Similarity to an Ideal Solution (TOPSIS). The Entropy method is applied to obtain objective weights and the improved TOPSIS method is used to rank the alternatives. The simulation results show that the proposed technique can make the distribution of weights more reasonable, and effectively reduce the number of handoffs.                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
            "|8600013|Robust offline trained neural network for TDOA based sound source localization                                  |\"Microphones\",\"Artificial neural networks\",\"Position measurement\",\"Training\",\"Neurons\",\"Noise measurement\",\"Microwave integrated circuits\"|Passive sound source localization using time-difference-of-arrival (TDOA) measurements is a challenging non-linear inversion problem. This paper investigates a new data-driven approach to SSL using TDOA measurements. A neural network (NN) is viewed as an architecture constrained non-linear function, with its parameters learnt from the training data. The NN is trained to learn the non-linear mapping between TDOA measurements and source location from the training data. Experimentally, we show that, NN trained even on noise-less TDOA measurements can achieve good performance for noisy TDOA inputs also. These performances are better than the traditional spherical interpolation (SI) method. Furthermore, we demonstrate that the NN model trained offline using simulated TDOA measurements outperforms the SI method for localizing real-life speech signals inside a simulated enclosure.                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
            "|8600029|A Social Bots Detection Model Based on Deep Learning Algorithm                                                  |\"Feature extraction\",\"Metadata\",\"Detection algorithms\",\"Measurement\",\"Deep learning\",\"Social network services\",\"Data mining\"              |With the development of the Internet, social bots are increasingly spreading on social platforms. Detecting these accounts that pose a threat to social networks has become an essential task that requires an effective detection algorithm. In this paper, a social bots detection model based on deep learning algorithm (DeBD) is proposed. The model mainly includes three layers. The first layer is the joint content feature extraction layer, which focuses on the extraction of features from tweets' content and their relationships. The second layer is the tweet metadata temporal feature extraction layer, which uses tweet metadata as temporal information to extract user social activity temporal features through LSTM. The third layer is the feature fusion layer, which integrates the extracted joint content features with the temporal features to detect social bots. We evaluated the proposed DeBD model on three different types of social bot data sets from the real world, and the experimental results demonstrate the model's effectiveness.                                                                                                                                                                                                                                                                                                  |\n",
            "|8600036|Pedestrian Detection and Attribute Analysis Program Based on CNN                                                |\"Training\",\"Image color analysis\",\"Detectors\",\"Feature extraction\",\"Semantics\",\"Monitoring\",\"Computational modeling\"                      |In recent years, deep learning object detectors including Fast/Faster R-CNN, SSD, R-FCN and Mask R-CNN have shown significant performance for general object detection except for pedestrians. The Region Proposal Network (RPN) in Faster R-CNN works well yet lacks of adaptability. Therefore, we suggest an adaptive real-time pedestrian detection and attribute identification scheme based on Caffe. The first contribution is the Adaptive Threshold Adjustment (ATA) algorithm for intelligent monitoring, which adjusts the threshold using pedestrian movement information. Moreover, to overcome the time-consuming defect, we analyze the influences of the number of layers, the size of convolution kernels and the number of feature maps to reduce redundant computation while maintaining satisfactory performance. By optimizing the neutral network structure, selecting model parameters, and data augmentation, we obtained a stable and well-performed model with fast detection rates and high accuracy. Furthermore, pedestrian information can be identified in our program, providing better service in security monitoring, intelligent robots, and other fields. Extensive experimental results demonstrate that even in complex and athletic scenarios, our method can make quality and speed improvements over current state-of-the-art techniques.|\n",
            "|8600037|Nonlinear State Estimation Technique Implementation for Human Heart Model                                       |\"Heart\",\"Mathematical model\",\"Muscles\",\"Noise measurement\",\"Pacemakers\",\"Kalman filters\",\"Nonlinear systems\"                              |Human heart is a vital organ therefore proper diagnosis of heart activities is essential. To estimate heart parameters, various parameter estimation techniques have been developed. In this work, we use Ensemble Kalman Filter (EnKF) and Particle Filter (PF) for dynamic assimilation of human heart parameters. EnKF and PF are modified filters specifically designed for state prediction of nonlinear systems with large data samples. A third-order mathematical heart model was employed to estimate three heart parameters, including movements of heart muscle fiber, tension in heart muscle, and electrochemical activity of the heart. The EnKF and PF were applied to the heart model, and different case studies were performed to observe the prediction accuracy by comparing the sum squared error values. The case studies were conducted with variable state and measurement noise values. The proposed approach demonstrated promising results in accurately predicting the human heart parameters.                                                                                                                                                                                                                                                                                                                                                        |\n",
            "|8600061|Metric Learning Algorithm Based on Weighted Pairwise Constrained Component Analysis for Person Re-identification|\"Euclidean distance\",\"Kernel\",\"Feature extraction\",\"Conferences\",\"Cameras\",\"Computer vision\"                                              |Currently, the field of computer vision widely uses the distance function to learn image pairs, with the Euclidean distance being the most commonly used method. But traditional Euclidean distance has disadvantage of distinguishing ability in the feature similarity measure. To address this issue, we propose a new algorithm called weighted Pairwise Constrained Component Analysis (wPCCA) for person reidentification (Re-ID), which is based on the weighted Euclidean distance. The wPCCA algorithm builds upon the PCCA and improves its measurement of characteristics using the weighted Euclidean distance. The experiments were conducted on two challenging datasets named i-LIDS and CAVIAR, and gained good results.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
            "|8600065|Identification of Jamming Factors in Electronic Information System Based on Deep Learning                       |\"Jamming\",\"Convolution\",\"Information systems\",\"Kernel\",\"Receivers\",\"Deep learning\",\"Training\"                                             |Jamming identification is the precondition of taking targeted anti-jamming measures, and it is very important to improve the adaptability of electronic information system to electromagnetic environment. While the most commonly used method of jamming recognition relies on expert knowledge-based feature extraction, the varied patterns and parameters of jamming can make it difficult to determine the correct feature set. Therefore, this paper introduces a deep learning approach, which automatically extracts features from the original data to identify the jamming factors of electronic information system. In order to demonstrate the effectiveness and practicability of this approach, the noise jamming factor identification of the superheterodyne receiver is introduced.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
            "|8600068|Person Tracking and Frontal Face Capture with UAV                                                               |\"Face\",\"Feature extraction\",\"Drones\",\"Target tracking\",\"Face detection\",\"Image color analysis\"                                            |This paper presents a novel approach to track a walking person and automatically capture a frontal photo using an unmanned aerial vehicle (UAV). The proposed method consists of three main parts, namely person detection and recognition, face detection and feature points localization, and vision-based UAV control. The person tracking module employs the YOLOv3 deep neural network for detecting the target person, and utilizes the Locality-constrained Linear Coding (LLC) algorithm for matching the target person. In terms of frontal face perception, we use Multi-task Cascaded Convolutional Neural Networks (MTCNN) for face detection. Based on the vision information obtained from the two modules, the UAV can fly around the target person and obtain the target's frontal face image. The outdoor experiments based on a Parrot Bebop2 drone verify the effectiveness and practicability of our method.                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
            "+-------+----------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for keywords for in-depth data analysis\n",
        "# List three example keywords\n",
        "keyword_1 = \"speech\"\n",
        "keyword_2 = \"image\"\n",
        "keyword_3 = \"text\""
      ],
      "metadata": {
        "id": "ejsVoly6HUmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, lower\n",
        "\n",
        "# Check keyword_1\n",
        "fusion.filter(lower(col(\"keyword\")).like(f\"%{keyword_1}%\")).show(10, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsGWHNsAGhxM",
        "outputId": "cbe22040-855e-4dab-f01a-e1029476e03d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|id     |title                                                                                                                                    |keyword                                                                                                                                                         |abstract                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
            "+-------+-----------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|8600091|Improved Epoch Extraction Using Variational Mode Decomposition Based Spectral Smoothing of Zero Frequency Filtered Emotive Speech Signals|\"Smoothing methods\",\"Estimation\",\"Speech processing\",\"Frequency estimation\",\"Time-frequency analysis\",\"Resonant frequency\"                                      |The objective of the present work is to improve the epoch extraction performance from emotive speech by proposing a post processing approach to the conventional zero frequency filtering (ZFF) method using variational mode decomposition (VMD) based spectral smoothing. Identifying the epochs in emotional speech signals is a challenging task due to the fast and uncontrolled variations of pitch. In the proposed method, the spectra of the short frames of zero frequency filtered signal (ZFFS) is subjected variational mode decomposition to get component spectra in five modes. A smoothed short-time spectra is generated by eliminating the spectra from the two higher VMD modes that contain high spectral variations. The modified ZFFS is then reconstructed using the sinusoidal parameters corresponding to single dominant frequency present in the smoothed spectra using VMD by parameter interpolation based sinusoidal synthesis. The resulting re-synthesized ZFFS has reduced spurious zero crossings as compared to that obtained from the conventional ZFF method for emotive speech signals. This is evident from the improved epoch identification accuracy and rate for all the emotive utterances (with 7 emotions) in the German emotion speech database with simultaneous speech and electroglottographic (EGG) signal recordings. The performance of the proposed method is found to be better or comparable with the other existing ZFF based post processing methods proposed for emotive speech signals in terms of the epoch identification accuracy with respect to the corresponding reference epochs estimated from EGG signals.|\n",
            "|8600131|Cell-Phone Identification from Recompressed Audio Recordings                                                                             |\"Mel frequency cepstral coefficient\",\"Feature extraction\",\"Audio recording\",\"Forensics\",\"Social network services\",\"Speech recognition\"                          |Audio forensics has numerous applications that can benefit from the ability to classify audio recordings based on the device they originated from, especially on social media platforms where huge amounts of data are posted daily. This paper utilizes passive signatures associated with the recording devices, as extracted from recorded audio itself, in the absence of any extrinsic security mechanism such as digital watermarking, to identify the source cell-phone of recorded audio. This approach uses device-specific information present in both low and high-frequency regions of the audio recordings, even in the absence of any extrinsic security mechanism like digital watermarking. The proposed system achieves a closed set accuracy of 97.2% on the only publicly available dataset in this field, MOBIPHONE, matching the state-of-the-art accuracy reported for this dataset. Furthermore, the proposed methodology outperforms existing methods by 4% in average accuracy on audio recordings which have undergone double compression, which frequently occurs when a recording is shared on social media.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
            "|8600157|Throat Microphone Speech Enhancement via Progressive Learning of Spectral Mapping Based on LSTM-RNN                                      |\"Speech enhancement\",\"Training\",\"Bandwidth\",\"Logic gates\",\"Microphones\",\"Task analysis\",\"Recurrent neural networks\"                                             |In this paper, we propose a progressive spectral mapping learning algorithm for throat microphone (TM) speech enhancement. Unlike previous full-band spectra mapping algorithms, this algorithm divides the spectra mapping from TM speech to Air-conducted (AC) speech into two tasks, one is the voice conversion task, and the other is the artificial bandwidth extension task. Long short-term memory recurrent neural network (LSTM-RNN) is further deployed as the mapping model. Objective evaluation results show that the TM speech quality is improved when compared with conventional full-band spectra mapping framework and DNN-based mapping model.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
            "|8600307|Robust Hierarchical Learning for Non-Negative Matrix Factorization With Outliers                                                         |\"Hidden Markov models\",\"Data models\",\"Linear programming\",\"Robustness\",\"Dictionaries\",\"Speech processing\",\"Convergence\"                                         |Desirable properties of extensions of non-negative matrix factorization (NMF) include robustness in the presence of noises and outliers, ease of implementation, the guarantee of convergence, operation in an automatic fashion that trades off the balance between data approximation and model simplicity well, and the capability to model the inherently sequential structure of time-series signals. The state-of-the-art methods typically have only a subset of these aforementioned properties and seldom simultaneously possess them all. In this paper, we propose a novel approach that provides all these desirable properties by extending the automatic relevance determination framework in NMF from Tan and Févotte. Using an objective function derived from maximum a posteriori estimation of a Bayesian model, the authors develop majorization-minimization algorithms that effectively determine the correct model order, despite the impact of noise and outliers. Additionally, the proposed algorithms are rigorously analyzed for convergence. The authors also incorporate convolutive bases in the model to capture the temporal continuity of data. We perform experiments on both synthetic and real-world data sets to show the efficiency and robustness of our approach.                                                                                                                                                                                                                                                                                                                                                                     |\n",
            "|8604332|Energy-efficient MFCC extraction architecture in mixed-signal domain for automatic speech recognition                                    |\"Mel frequency cepstral coefficient\",\"Feature extraction\",\"Computer architecture\",\"Frequency-domain analysis\",\"Energy efficiency\",\"Automatic speech recognition\"|This paper presents a new processing architecture for automatic speech recognition that focuses on the efficient extraction of Mel-Frequency Cepstrum Coefficients (MFCC). Inspired by the human ear, the energy-efficient analog-domain information processing is adopted to replace the energy-intensive Fourier Transform in conventional digital-domain. In addition, this proposed architecture extracts acoustic features in mixed-signal domains, which helps to reduce the cost of Analog-to-Digital Converters (ADCs) and computational complexity. Circuit-level simulation based on 180nm CMOS technology shows that the proposed architecture can achieve an energy consumption of 2.4 nJ/frame and a processing speed of 45.79 μs/frame. The proposed architecture achieves 97.2% energy saving and about 6.4× speedup than state of the art. Speech recognition simulation reaches the classification accuracy of 99% using the proposed MFCC features.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
            "|8604460|Spectral Masking in MFCC Calculation for Noisy Speech                                                                                    |\"Mel frequency cepstral coefficient\",\"Transform coding\",\"Estimation\",\"Harmonic analysis\",\"Psychoacoustic models\",\"Speech recognition\",\"Standards\"               |To increase noise immunity of the MFCC (mel-frequency cepstral coefficients) widely used for voice signal parametrisation it is suggested to use a psychoacoustic model of frequency masking. Furthermore, by considering the formation mechanism of formant regions in the voice signal spectrum, the spectral samples that correspond to multiple harmonics of the fundamental tone could also be modified. The modified algorithm is investigated on the basis of the single word recognition system adapted for MFCC voice signal parametrisation only. The positive effect of using proposed additional voice signal transformation in the parametrisation algorithm is shown.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
            "|8606862|Lead Engagement by Automated Real Estate Chatbot                                                                                         |\"Computer science\",\"Itemsets\",\"Speech processing\",\"Deep learning\",\"Natural languages\",\"Tagging\"                                                                 |In recent times, automated chatbots have become a growing trend in the real estate industry. While they may not entirely replace the traditional interactions between real estate agents and home buyers, chatbots can be instrumental in engaging potential clients in meaningful conversations, thus proving to be highly effective for lead capture. The paper aims to introduce an intelligent chatbot built specifically for this purpose. Various machine learning techniques, including multi-task deep learning technique for intent identification and frequent itemsets for conversation elaboration, have been employed in our system. Our chatbot has been deployed by CEO K35 GROUP JSC with daily updated data of real estate information at Hanoi and Ho Chi Minh cities, Vietnam.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
            "|8606997|Listening and Grouping: An Online Autoregressive Approach for Monaural Speech Separation                                                 |\"Deep learning\",\"Speech processing\",\"Training\",\"Neural networks\",\"Image analysis\",\"Extraterrestrial measurements\",\"Task analysis\"                               |This paper presents a novel autoregressive method for multi-speaker monaural speech separation utilizing deep learning. It exploits a causal temporal context in both mixture and past estimated separated signals and performs online separation that is compatible with real-time applications. The method utilizes a learned listening and grouping architecture inspired by computational auditory scene analysis, incorporating a grouping stage that effectively addresses the label permutation problem at both frame and segment levels. Experimental results on the WSJ0-2mix benchmark show that the new approach can achieve better signal-to-distortion ratio and perceptual evaluation of speech quality scores than most of the state-of-the-art methods for both closed-set and open-set evaluations, even methods that exploit whole-utterance statistics for separation. Combining these results with the fact that the approach requires fewer model parameters highlights the effectiveness of this method.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
            "|8607038|Effective Combination of DenseNet and BiLSTM for Keyword Spotting                                                                        |\"Feature extraction\",\"Speech recognition\",\"Time series analysis\",\"Hidden Markov models\",\"Google\",\"Spectrogram\",\"Recurrent neural networks\"                      |Keyword spotting (KWS) plays a crucial role in human-computer interaction for smart on-device terminals and service robots. In this paper, based on the powerful ability of DenseNet on extracting local feature-maps, we propose a new network architecture (DenseNet-BiLSTM) for KWS. In our DenseNet-BiLSTM, the DenseNet is primarily applied to obtain local features, while the BiLSTM is used to grab time series features. Our approach employs the DenseNet for extracting local features and incorporates BiLSTM to capture temporal features. However, the standard DenseNet may overlook the contextual information for speech audio, which can disrupt its effectiveness. Therefore, we introduce a new variant called DenseNet-Speech, which retains the temporal details by eliminating the pooling on the time dimension in transition layers. The experimental results show that feature-maps from DenseNet-Speech maintain time series information well. The experimental results demonstrate that our approach surpasses the state-of-the-art approaches in terms of accuracy, achieving a recognition accuracy of 96.6% for the 20-commands recognition task with just 223K trainable parameters. In summary, our DenseNet-BiLSTM architecture with a speech-oriented DenseNet helps in the accurate identification of keywords while maintaining efficiency.                                                                                                                                                                                                                                                                                              |\n",
            "|8607053|Sequence-to-Sequence Acoustic Modeling for Voice Conversion                                                                              |\"Acoustics\",\"Feature extraction\",\"Vocoders\",\"Speech processing\",\"Cloning\",\"Decoding\",\"Linguistics\"                                                              |In this paper, we introduce SCENT, a neural network designed for acoustic modeling in voice conversion. At training stage, a SCENT model is estimated by aligning the feature sequences of source and target speakers implicitly using attention mechanism. At the conversion stage, acoustic features and durations of source utterances are converted simultaneously using the unified acoustic model. Our model uses Mel-scale spectrograms as acoustic features, which capture both excitation and vocal tract descriptions of speech signals. The bottleneck features extracted from source speech using an automatic speech recognition model are appended as an auxiliary input. Finally, we use a WaveNet vocoder conditioned on Mel-spectrograms to reconstruct waveform outputs. Notably, our method achieves better duration conversion compared to conventional methods. Experimental results show that our proposed method obtained better objective and subjective performance than the baseline methods using Gaussian mixture models and deep neural networks as acoustic models. This proposed method also outperformed our previous work, which achieved the top rank in Voice Conversion Challenge 2018. Ablation tests further confirmed the effectiveness of several components in our proposed method.                                                                                                                                                                                                                                                                                                                                                   |\n",
            "+-------+-----------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check keyword_2\n",
        "fusion.filter(lower(col(\"keyword\")).like(f\"%{keyword_2}%\")).show(10, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTShS9oVININ",
        "outputId": "e9a6d98d-e704-434b-97bb-4cbabf0e3070"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|id     |title                                                                                    |keyword                                                                                                                                      |abstract                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
            "+-------+-----------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|8600036|Pedestrian Detection and Attribute Analysis Program Based on CNN                         |\"Training\",\"Image color analysis\",\"Detectors\",\"Feature extraction\",\"Semantics\",\"Monitoring\",\"Computational modeling\"                         |In recent years, deep learning object detectors including Fast/Faster R-CNN, SSD, R-FCN and Mask R-CNN have shown significant performance for general object detection except for pedestrians. The Region Proposal Network (RPN) in Faster R-CNN works well yet lacks of adaptability. Therefore, we suggest an adaptive real-time pedestrian detection and attribute identification scheme based on Caffe. The first contribution is the Adaptive Threshold Adjustment (ATA) algorithm for intelligent monitoring, which adjusts the threshold using pedestrian movement information. Moreover, to overcome the time-consuming defect, we analyze the influences of the number of layers, the size of convolution kernels and the number of feature maps to reduce redundant computation while maintaining satisfactory performance. By optimizing the neutral network structure, selecting model parameters, and data augmentation, we obtained a stable and well-performed model with fast detection rates and high accuracy. Furthermore, pedestrian information can be identified in our program, providing better service in security monitoring, intelligent robots, and other fields. Extensive experimental results demonstrate that even in complex and athletic scenarios, our method can make quality and speed improvements over current state-of-the-art techniques.|\n",
            "|8600068|Person Tracking and Frontal Face Capture with UAV                                        |\"Face\",\"Feature extraction\",\"Drones\",\"Target tracking\",\"Face detection\",\"Image color analysis\"                                               |This paper presents a novel approach to track a walking person and automatically capture a frontal photo using an unmanned aerial vehicle (UAV). The proposed method consists of three main parts, namely person detection and recognition, face detection and feature points localization, and vision-based UAV control. The person tracking module employs the YOLOv3 deep neural network for detecting the target person, and utilizes the Locality-constrained Linear Coding (LLC) algorithm for matching the target person. In terms of frontal face perception, we use Multi-task Cascaded Convolutional Neural Networks (MTCNN) for face detection. Based on the vision information obtained from the two modules, the UAV can fly around the target person and obtain the target's frontal face image. The outdoor experiments based on a Parrot Bebop2 drone verify the effectiveness and practicability of our method.                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
            "|8600419|On comparing color spaces for fabric defect classification based on local binary patterns|\"Image color analysis\",\"Fabrics\",\"Support vector machines\",\"Inspection\",\"Feature extraction\",\"Databases\"                                     |There is a numerous color spaces with different properties in literature. To find the appropriate and relevant color space for the fabric defect classification problem, we propose investigating the performance and robustness of the Local Binary Pattern (LBP) descriptor in a supervised context, using an SVM classifier. The experimental results show that the luminance-chrominance spaces are suitable for coding fabric defect with the classification accuracy obtained is 92.1%.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
            "|8600437|Visualization of Railway Scene Classification Model via Grad-CAM                         |\"Rail transportation\",\"Image analysis\",\"Feature extraction\",\"Visual databases\",\"Task analysis\",\"Visualization\"                               |Railway detection tasks require a large number of images, but the lack of effective image classification methods makes it challenging to analyze detection images deeply. Using convolutional neural networks (CNN) to realize railway image scene classification is an effective technical means. This study presents a method for reducing database bias using Gradient-weighted Class Activation Mapping (Grad-CAM) to enhance scene classification accuracy, achieving an accuracy rate of 95.3% (top3) on Railway12 database. Our approach combines two key insights: (1) the limited amount of railway scene database makes it difficult for CNN to achieve high performance, so we transfer pre-trained ImageNet-CNN for fine-tuning on railway scene database; (2) we introduce the Grad-CAM visualization method to analyze the model's classification pattern and intuitively display possible database biases, providing a clear strategy for reducing dataset bias.                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
            "|8600447|Tone Correction with Quasi-cubic Splines in a Spherical Color Model                      |\"Image color analysis\",\"Splines (mathematics)\",\"Mathematical model\",\"Color\",\"Computational modeling\",\"Azimuthal angle\",\"Safety\"              |In this paper, the focus is on designing cubic splines as tone correction functions for the achromatic component of a spherical color model. This model is advantageous as it provides more perceptually smooth color changes along its coordinates compared to commonly used color models HSV and HSL. However, using a tone correction function in this model can lead to gamut issues due to the higher number of color points it contains compared to the RGB color model. The paper demonstrates the gamut issue can be avoided when tone correction functions are well designed and the general tone correction techniques still work well in the spherical color model. A particular type of cubic splines is designed to serve the purpose, and these splines can be adopted by the general tone correction techniques including those for low-key, middle-key and high-key images with correspondingly selected parameters. Experimental results demonstrate that these cubic splines work well for tone correction in the spherical color model, ensuring that the tone correction technique seamlessly integrates with this model.                                                                                                                                                                                                                                     |\n",
            "|8600458|A Fast CTU Depth Selection Algorithm for H.265/HEVC Based on Machine Learning            |\"Channel coding\",\"Distortion\",\"Complexity theory\",\"Rate distortion theory\",\"Machine learning\",\"Image coding\"                                 |This paper presents a fast depth selection algorithm for CTU (frame coding units) based on machine learning. The algorithm addresses the lack of depth discrimination in initial division of coding units and the inefficiencies of the classifier's input feature selection. Firstly, the paper designs an initial division depth prediction strategy based on texture complexity and quantization parameters to skip nonessential sizes of coding units. Secondly, the input characteristics of the classifier are determined based on the relationship between bit-rate and distortion, and a selection strategy for the termination depth of coding units is designed. By skipping the calculation process of the time-consuming rate distortion cost, the ending dividing depth of the current coding unit can be judged in advance and accelerate the process of the inter-frame coding. The proposed algorithm decreases frame encoding time by 34.56%, while maintaining accuracy compared with HM-15.0.                                                                                                                                                                                                                                                                                                                                                                  |\n",
            "|8600472|Underwater object Images Classification Based on Convolutional Neural Network            |\"Training\",\"Image segmentation\",\"Feature extraction\",\"Convolutional neural networks\",\"Image classification\",\"Support vector machines\",\"Sonar\"|To address the challenge of underwater object image classification with limited training data availability, a new classification approach based on Convolutional Neural Network (CNN) is proposed. Firstly, an advanced method of Markov random field-Grabcut algorithm is adopted to segment images into two regions: shadow and sea-bottom. To account for the specific properties of the data, a CNN model is constructed with two parts: a convolutional part and a classification part, following the structure of Alexnet. Finally, the transfer learning technique is employed to train the CNN model to classify three distinct shapes of underwater objects (cylinder, truncated cone, and sphere). The method is validated on synthetic aperture sonar (SAS) datasets, and its accuracy is compared to that of Support Vector Machine (SVM) and CNN models that leverage only trial data. The proposed method achieves superior accuracy compared to the SVM and CNN models with limited training data, demonstrating its effectiveness in addressing the challenge of underwater object image classification.                                                                                                                                                                                                                                                          |\n",
            "|8600485|Towards 3D Human Action Recognition Using a Distilled CNN Model                          |\"Training\",\"Feature extraction\",\"Mathematical model\",\"Joints\",\"Image color analysis\",\"Predictive models\"                                     |Motivated by the remarkable performance achieved using deep learning strategies in solving action recognition tasks, an effective, yet simple method is proposed for encoding the spatiotemporal information of skeleton sequences into color texture images, referred to as Skeletal Optical Flows (SOFs). SOFs capture meaningful temporal information by representing the kinetic energy, predefined angles, and pair-wise displacements between joints over consecutive frames of skeleton data as color variations, thus making them highly interpretable. To exploit the discriminative features of SOFs for human action recognition, we employ a novel Convolutional Neural Network with Correctness-Vigilant Regularizer (CVR-CNN). Empirical results demonstrate the superior efficiency of our proposed method in terms of the generalizability of the generated model, training convergence speed, and classification accuracy on commonly used action recognition datasets such as MHAD, HDM05, and NTU RGB+D.                                                                                                                                                                                                                                                                                                                                                       |\n",
            "|8600501|Multi-frame Image Super Resolution with Natural Image Prior                              |\"Image resolution\",\"Image reconstruction\",\"Bayes methods\",\"Interpolation\",\"Noise level\",\"Noise measurement\",\"Computational modeling\"         |A new algorithm for multi-frame image super resolution (SR) has been proposed, which utilizes Bayesian modeling with natural image prior modeled by fields of experts (FoE). Multi-frame SR can be used to obtain a high resolution (HR) image from a set of degraded low resolution (LR) images without changing any hardware device. However, SR is well known to be an ill-posed problem. So state-of-the-art solutions usually formulate the problem with Bayesian modeling techniques, which infer the HR image based on not only the LR input images but also on prior information about the HR image. Nonetheless, majority of the Bayesian SR approaches utilize simple prior models such as L1 norm, TV prior, and Laplacian prior, which do not exploit the statistics of natural scenes accurately. In this paper, a Bayesian multi-frame image SR approach using a FOE model as the prior for natural images is presented. The Maximum a Posteriori (MAP) framework is employed to estimate the HR image. The proposed method cannot only capture the statistics of natural images well, but also require less computational power than the other Bayesian modelling methods such as Sampling methods and Approximate inference. The proposed method shows superior or comparable results to the state-of-art multi-frame SR methods.                                 |\n",
            "|8600506|Dynamic Multi-mapping Convolutional Network for Image Super-Resolution                   |\"Interpolation\",\"Image reconstruction\",\"Image resolution\",\"Convolution\",\"Feature extraction\",\"Convolutional codes\",\"Aerodynamics\"            |The learning methods have recently achieved great success for single image super-resolution. Convolutional neural networks (CNNs) have proven to be robust, achieving state-of-the-art performance. In this paper, we propose a Dynamic Multi-mapping Convolutional Network (DMCN) that improve the SR performance. Rather than relying on fixed kernels like Bicubic interpolation, we utilize dynamic filters to resize the LR input in our pre-trained module. Based on an end-to-end manner, more accurate and effective features from middle layers can be learned. Additionally, our multi-mapping module provides extra information and high-frequency details, resulting in sharper, high-resolution images. Through extensive quantitative and qualitative evaluations, our algorithm effectively improves image resolution.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
            "+-------+-----------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check keyword_3\n",
        "fusion.filter(lower(col(\"keyword\")).like(f\"%{keyword_3}%\")).show(10, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQ31xWapIV3z",
        "outputId": "f1863e63-f211-4e74-baff-2b3a80fc5fad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|id     |title                                                                                                                      |keyword                                                                                                                                                                        |abstract                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
            "+-------+---------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|8609600|LASAGNE: Locality and Structure Aware Graph Node Embedding                                                                 |\"Approximation algorithms\",\"Task analysis\",\"Training\",\"Machine learning algorithms\",\"Learning systems\",\"Context modeling\",\"Graph theory\"                                       |In this work, we introduce LASAGNE, a method for unsupervised learning of graph node embeddings that takes into account locality and structure awareness. In particular, we show that the performance of existing random-walk based approaches depends strongly on the structural properties of the graph, e.g., the size of the graph, whether the graph has a flat or upward-sloping Network Community Profile (NCP), whether the graph is expander-like, whether the classes of interest are more k-core-like or more peripheral, etc. For larger graphs with flat NCPs that are highly expander-like, the quality of the vector representations obtained through these methods is often lower due to the rapid expansion of random walks touching dissimilar nodes. LASAGNE circumvents this issue by utilizing localized Approximate Personalized PageRank stationary distributions to incorporate more precise local information into node embeddings, rather than relying on global random walks or fixed hop distances. This leads, in particular, to more meaningful and more useful vector representations of nodes in poorly-structured graphs. We show that LASAGNE leads to significant improvement in downstream multi-label classification for larger graphs with flat NCPs and that it is comparable for smaller graphs with upward-sloping NCPs.|\n",
            "|8611085|Word Representation With Salient Features                                                                                  |\"Semantics\",\"Computational modeling\",\"Context modeling\",\"Predictive models\",\"Task analysis\",\"Analytical models\",\"Computational efficiency\"                                     |Inspired from the idea that the contexts in which a word occurs are of different significance, this paper proposes a novel method, called word representation with Salient Features (SaFe), to represent words using salient features selected from the context words. The SaFe method employs the point-wise mutual information (PMI) method with scaled context window to measure word association between a target word and its context. The number of salient features for a given word is determined by the ratio between the number of unique contexts and the total occurrences in the corpus. The SaFe approach can be applied to the positive PMI matrix (PPMI) and can be further decomposed using truncated singular vector decomposition to obtain dense vectors. Experimental results demonstrate that SaFe-PPMI achieves remarkable improvements in seven semantic relatedness tasks and outperforms state-of-the-art models. Besides being computationally efficient, SaFe provides a powerful tool for representing words in natural language processing applications.                                                                                                                                                                                                                                                                           |\n",
            "|8611614|News Credibility Measure Utilizing Ontologies &amp; Semantic Weighing Schemes (NCMOSWS)                                    |\"Ontologies\",\"Semantics\",\"Large scale integration\",\"Text categorization\",\"Feature extraction\",\"Syntactics\",\"Weight measurement\"                                                |The prevalence of media news sources has increased significantly in recent years, leading to a growing concern about the credibility of news content. This brings to the service news credibility issue which emerges to become one of the most important issues concerning assessment of news items with respect to either the news editors' personnel or the readers of news items. In an effort to address this issue, we have developed a model known as NCMOSWS that leverages established semantic weighing methods and ontology to accurately assess the credibility of news items. We apply our model on the RSS of a set of news agencies and show that the model is highly reliable on measuring partial credibility of news items.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
            "|8611786|Towards Physical Distortion Identification and Removal in Document Images                                                  |\"Distortion\",\"Optical character recognition software\",\"Noise reduction\",\"Optical distortion\",\"Text analysis\",\"Object recognition\",\"Feature extraction\"                         |Physical distortions alongside digital artefacts are frequently observed in document images. Their presence sabotages the optical character recognition (OCR) process which not only leads to a reduced amount of automatically retrievable content, but also deteriorates the performance of other document analysis algorithms that rely on layout analysis or content recognition. In this study, a method for identifying and eliminating specific types of physical distortions from document images is proposed. By exploiting the intensity and spatial relation of distorted pixels, we construct a conditional random field (CRF) based method for distortion identification. Additionally, a peak searching approach is introduced to learn the energy functions' model parameters in the CRF model from the image automatically. Discrimination between pixels originating from the genuine document content and those related to physical noise is achieved by maximizing the CRF model's conditional probability. Real-life image samples illustrate the efficacy of the proposed method.                                                                                                                                                                                                                                                           |\n",
            "|8612617|Text Categorization by Weighted Features                                                                                   |\"Task analysis\",\"Semantics\",\"Feature extraction\",\"Sports\",\"Deep learning\",\"Text categorization\",\"Measurement\"                                                                  |Text representation is a key task in machine learning, allowing for the conversion of varying lengths of text into feature vectors. While early methods used discrete and sparse lexical and syntactic features, these approaches struggled to capture the semantic relationships between words. Recent advances in deep learning, which represents text segments into dense and continuous vectors, has shed light on this problem. However, the main limitation is they are usually based on complex neural network structure, which are resource-consuming to train and make inference. To address this issue, we propose a novel approach to text representation that considers the importance of words on both a local and global scale using the BM25 weighing schema. We use word vectors pretrained from large text corpus to capture the latent semantic relatedness between words. Experimental results show that our approach is effective and efficient compared with existing feature-based, unsupervised and supervised baselines.                                                                                                                                                                                                                                                                                                                 |\n",
            "|8615845|Feature-Level Fusion using Convolutional Neural Network for Multi-Language Synthetic Character Recognition in Natual Images|\"Character recognition\",\"Feature extraction\",\"Text recognition\",\"Image recognition\",\"Shape\",\"Kernel\",\"Optical character recognition software\"                                  |This paper proposes a new CNN architecture for recognizing synthetic Urdu and English characters in natural scene images. The proposed method involves extracting features using three separate sub-models of the CNN and fusing them in a single feature vector. The network is trained exclusively on synthetic character images of English and Urdu texts in natural images. The Chars74k-Font dataset is utilized for English text, while the synthetic dataset is created for Urdu text by automatically cropping image patches from four background image datasets and then placing random characters in the image patch. The performance of the network is evaluated on a combined synthetic dataset of Urdu and English characters, as well as the separate synthetic datasets of Urdu and English characters. The experimental results show that the network performs well on synthetic datasets.                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
            "|8616891|Story Ending Selection by Finding Hints From Pairwise Candidate Endings                                                    |\"Task analysis\",\"Context modeling\",\"Predictive models\",\"Speech processing\",\"Computational modeling\",\"Natural languages\",\"Semantics\"                                            |The ability of story comprehension is a strong indicator of natural language understanding. However, a new challenge has arisen in the form of the Story Cloze Test, which requires machines to select the correct ending from two possible endings in the context of a four-sentence story. Most existing methods for Story Cloze Test are essentially matching-based that operate by comparing an individual ending with a given context, therefore suffering from the evidence bias issue: both candidate endings can obtain supporting evidence from the story context, which misleads the classifier to choose an incorrect ending. To address this issue, we present a novel idea to improve story comprehension by utilizing the hints that are obtained through comparing two candidate endings. Our model anticipates a feature vector for a plausible ending based on the context alone and refines this prediction using hints that encode the difference between the two possibilities. The candidate ending whose feature vector is more similar to the predicted ending vector is regarded as correct. Experimental results demonstrate that our approach can alleviate the evidence bias issue and improve story comprehension.                                                                                                                   |\n",
            "|8617768|CFC-ITS: Context-Aware Fog Computing for Intelligent Transportation Systems                                                |\"Cloud computing\",\"Edge computing\",\"Internet of Things\",\"Context-aware services\",\"Time factors\",\"Intelligent transportation systems\",\"Autonomous vehicles\",\"Connected vehicles\"|This paper proposes a novel context-aware fog computing framework for intelligent transportation systems (ITS) called CFC-ITS. This scheme consists of multiple intelligent tiers: Internet of Things tier, fog service tier, and global cloud service tier supporting edge analytics for ITS services in a connected car environment. Each tier on the fog senses different contexts like location, time, available resources, and estimated response time for efficiently processing tasks to provide delay-sensitive services while optimizing virtualized resources. A preliminary prototype and a testbed for this study were built to validate the robustness of the proposed approach.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
            "|8618799|Metaheuristic and Evolutionary Methods for Feature Selection in Sentiment Analysis (a Comparative Study)                   |\"Feature extraction\",\"Sentiment analysis\",\"Text categorization\",\"Filtration\",\"Classification algorithms\",\"Tagging\",\"Wrapping\"                                                  |With the accelerated evolution of World Wide Web and the widespread of on-line collaborative tools, there is an increasing care towards automatic tools for Sentiment Analysis to provide a quantitative measure of “positivity” or “negativity” about opinions or social comments. However, there are several challenges faced in the process of sentiment analysis and evaluation, which hinder the accurate interpretation of sentiments and detection of suitable sentiment polarity. These challenges become obstacles in analyzing the accurate meaning of sentiments and detecting the suitable sentiment polarity. The most important challenge is to identify and extract features we will use in our model. It serves as a quick reference guide in selecting the most suitable methods for solving specific problems in the sentiment analysis field, particularly in the feature selection stage.                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
            "|8619598|Controllability of Large-Scale Networks: An Output Controllability Approach                                                |\"Controllability\",\"Computational modeling\",\"Large-scale systems\",\"Control design\",\"Context modeling\",\"Aggregates\"                                                              |In this paper, we study the controllability and energy consumption issues in large-scale networks. Instead of controlling each node individually, we focus on controlling an output that is defined as a measurement (such as the average) of the nodes that are not directly controlled. We introduce the concept of Output Controllability and the Output Controllability Gramian to analyze the behavior of the system. In this context, we show that it is possible to obtain a reduced-order approximated model which makes the Gramian computation and control design much easier. Our simulations indicate that the reduced model is consistent with the original one and more robust for lower ratios of controlled nodes.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
            "+-------+---------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**generation**"
      ],
      "metadata": {
        "id": "MtacG0QLI2D4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the structure of the table\n",
        "generation.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxxhelJjI5yu",
        "outputId": "4530ed2e-ae8c-41e4-b220-70a7bac53cae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- title: string (nullable = true)\n",
            " |-- keyword: string (nullable = true)\n",
            " |-- abstract: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Showcase the top five rows data\n",
        "generation.show(10, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOgDURKgI9ES",
        "outputId": "b495231c-62f2-40bb-91b2-11b766240365"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|id     |title                                                                                                       |keyword                                                                                                                                   |abstract                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
            "+-------+------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|8600003|An Improved Variable-Node-Based BP Decoding Algorithm for NAND Flash Memory                                 |\"Flash memories\",\"Reliability\",\"Decoding\",\"Parity check codes\",\"Convergence\",\"Error probability\",\"Threshold voltage\"                      |The reliability of flash memories is highly dependent on the accuracy of the decoding process. In this paper, an improved variable-node-based BP decoding algorithm is proposed for NAND flash memory. The algorithm utilizes parity check codes to improve the convergence and reduce the error probability. The threshold voltage is also taken into consideration to enhance the accuracy of the decoding process. The simulation results show that the proposed algorithm outperforms the traditional BP algorithm in terms of error correction capability and efficiency. The proposed algorithm has great potential to be applied in flash memory systems, which can significantly enhance the reliability and longevity of flash memories.                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
            "|8600004|Mobile Robot Location Algorithm Based on Improved Particle Filtering                                        |\"Sociology\",\"Statistics\",\"Simultaneous localization and mapping\",\"Genetic algorithms\",\"Estimation\",\"Filtering\"                            |This paper proposes an improved particle filtering algorithm for mobile robot location. The algorithm is based on the simultaneous localization and mapping technique, and incorporates a genetic algorithm to optimize the estimation process. The proposed algorithm improves filtering accuracy while reducing computational complexity, making it suitable for real-time robotic applications. In addition, the algorithm integrates sociological and statistical data to enhance robot localization accuracy, allowing for better overall robot performance. The results show that the proposed algorithm outperforms existing particle filtering methods, achieving higher accuracy and efficiency in robot location estimation. Overall, this research presents a valuable contribution to the development of robust and efficient mobile robot localization techniques.                                                                                                                                                                                                                                                                              |\n",
            "|8600008|Vertical Handoff Decision Algorithm for Heterogeneous Wireless Networks Based on Entropy and Improved TOPSIS|\"Entropy\",\"Handover\",\"Wireless networks\",\"Decision making\",\"Heterogeneous networks\"                                                       |This paper proposes a novel algorithm for vertical handoff decision making in heterogeneous wireless networks based on entropy and improved TOPSIS. With the increasing popularity of diverse wireless technologies, there is a need for an efficient and reliable handover mechanism to provide uninterrupted services to users. This algorithm employs entropy to measure the degree of randomness in the network and improve the accuracy of the handover decision. The proposed method also involves the improved TOPSIS algorithm to rank the candidate networks for handover. The results show that the proposed algorithm outperforms other existing algorithms in terms of handover success rate, handover delay, and network utilization. In conclusion, the entropy-based vertical handoff decision algorithm can effectively enhance the performance of heterogeneous wireless networks and provide seamless services to end-users.                                                                                                                                                                                                               |\n",
            "|8600013|Robust offline trained neural network for TDOA based sound source localization                              |\"Microphones\",\"Artificial neural networks\",\"Position measurement\",\"Training\",\"Neurons\",\"Noise measurement\",\"Microwave integrated circuits\"|This paper proposes a robust offline trained neural network for TDOA based sound source localization, which utilizes both artificial neural networks and position measurement. The proposed approach involves training the neural network by adjusting the weights and biases of its neurons, using a set of pre-collected noise and TDOA measurements from multiple microphones. The trained neural network is then used to predict the position of a sound source, based on the TDOA values obtained from the microphones. The accuracy of the proposed approach is confirmed through extensive noise measurement experiments, and compared favorably with other sound source localization techniques. The proposed approach is particularly suitable for use in noisy environments, where microwave integrated circuits and other conventional methods may be less effective. Overall, the proposed robust offline trained neural network for TDOA based sound source localization represents a significant advance in the field of sound source localization, and has important applications in fields such as acoustic surveillance and robotics.       |\n",
            "|8600014|Gaussian MAC with Feedback and Strictly Causal State Information                                            |\"Encoding\",\"Transmitters\",\"Decoding\",\"Indexes\",\"Additives\",\"Estimation error\",\"AWGN channels\"                                             |This paper proposes a method for encoding and decoding Gaussian multiple access channels (MAC) with feedback and strictly causal state information. The system consists of multiple transmitters that send signals to a receiver over a MAC. The transmitter is equipped with an encoder that maps the input symbols to the corresponding transmit signals. At the receiver end, the decoder uses a causal state estimator to estimate the state information that affects the transmitted signals. The decoder uses this information along with the received signals to estimate the transmitted symbols. The proposed method also requires the use of an index set and an additive error to minimize the estimation error. The system's performance is analyzed for AWGN channels, and the results show that the proposed system outperforms other existing approaches.                                                                                                                                                                                                                                                                                     |\n",
            "|8600018|Efficient Detection of Phishing Attacks with Hybrid Neural Networks                                         |\"Phishing\",\"Convolution\",\"Deep learning\",\"Convolutional neural networks\",\"Uniform resource locators\",\"Computer architecture\"              |Phishing attacks are a prevalent form of cybercrime, which can result in significant financial losses and data breaches. The identification of phishing attacks in real-time is essential to prevent potential harm caused by such attacks. In recent years, Deep learning models have proved to be a promising solution for detecting phishing attacks. Specifically, Convolutional neural networks (CNN) have achieved remarkable results in various computer vision tasks. In this paper, we propose a Hybrid Neural Network approach that leverages the strengths of both traditional machine learning and deep learning models. Our proposed approach uses Convolutional neural networks and traditional machine learning algorithms to extract meaningful features from Uniform Resource Locators (URL) and classify them as malicious or benign. Experimental results demonstrate that the proposed method outperforms the existing phishing detection methods in terms of accuracy and detection rate. Furthermore, the proposed method can adapt to various computer architectures and perform efficient detection of phishing attacks in real-time.|\n",
            "|8600029|A Social Bots Detection Model Based on Deep Learning Algorithm                                              |\"Feature extraction\",\"Metadata\",\"Detection algorithms\",\"Measurement\",\"Deep learning\",\"Social network services\",\"Data mining\"              |This paper presents a novel approach for detecting social bots, which are automated accounts created to mimic human behavior on social network services. The proposed model is based on a deep learning algorithm that utilizes feature extraction and metadata analysis to identify patterns in social media activity. Detection algorithms are employed to measure the deviation of a given account's behavior from that of a human, and to classify the account as a social bot or a human. The model utilizes data mining techniques to collect and analyze large amounts of social media data, and to build a comprehensive dataset of social bots for training and testing. The deep learning algorithm used in the proposed model is shown to achieve high accuracy in detecting social bots, as compared to traditional methods. This research has important implications for the detection and prevention of harmful activities on social media platforms, and could provide a valuable tool for social media providers to identify and remove malicious accounts.                                                                                  |\n",
            "|8600031|Energy-Efficient Network Coding Scheme for Two-Way Relay Visible Light Communications                       |\"Relays\",\"Network coding\",\"Maximum likelihood estimation\",\"Radio frequency\",\"Receivers\",\"Channel estimation\",\"Visible light communication\"|This paper proposes an energy-efficient network coding scheme for two-way relay visible light communications, which employs the concept of relays and maximum likelihood estimation (MLE) algorithm to improve the performance of the system. Network coding is used to combine the messages from the transmitters and generate new ones to be transmitted through the relay, thus increasing the data rate and reducing the energy consumption. The MLE algorithm is used for channel estimation, which improves the accuracy of the received signal and reduces interference from other sources such as radio frequency (RF) signals. The proposed scheme also includes a receiver design that allows for efficient decoding of the transmitted data. Simulations show that the proposed scheme outperforms traditional two-way relaying schemes and achieves high energy efficiency and improved channel estimation in visible light communication systems.                                                                                                                                                                                               |\n",
            "|8600032|Radio Classify Generative Adversarial Networks: A Semi-supervised Method for Modulation Recognition         |\"Modulation\",\"Training\",\"Signal to noise ratio\",\"Gallium nitride\",\"Generative adversarial networks\",\"Generators\",\"Deep learning\"          |This paper proposes a semi-supervised approach for Modulation Recognition using Radio Classify Generative Adversarial Networks. The method utilizes Generative Adversarial Networks (GANs) with two components - a generator and a discriminator. The generator is utilized to generate synthetic data while the discriminator is used to classify the data. The paper focuses on training the model, which uses a combination of labeled and unlabeled data. The proposed method achieves high accuracy while using fewer labeled data samples, effectively reducing the need for expensive data labeling. Additionally, the method can also improve the Signal to noise ratio of the signal. The paper also discusses the use of Gallium nitride and Deep learning in radio classification. Overall, the results demonstrate the effectiveness of the proposed approach and suggest possible ways to improve future modulation recognition tasks.                                                                                                                                                                                                          |\n",
            "|8600034|Secrecy Anti-jamming Game Learning in D2D Underlay Cellular Networks with an Active Eavesdropper            |\"Jamming\",\"Games\",\"Device-to-device communication\",\"Relays\",\"Cellular networks\",\"Physical layer security\",\"Interference\"                  |This paper presents a study on secrecy anti-jamming game learning in Device-to-Device (D2D) underlay cellular networks with an active eavesdropper. Jamming and interference are major challenges for secure and reliable wireless communication. Game theory is used in this study to model the interaction between the jammer and the eavesdropper as well as between the legitimate user and the relay node. The proposed scheme enhances the Physical Layer Security (PLS) by using relays to forward the signals. Games are played between the relay node and the eavesdropper to determine the best relay to use for maximizing secrecy performance. The study provides the necessary theoretical analysis and simulation results to verify the effectiveness of this approach for enhancing the secrecy of D2D underlay cellular networks.                                                                                                                                                                                                                                                                                                            |\n",
            "+-------+------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check keyword_1\n",
        "generation.filter(lower(col(\"keyword\")).like(f\"%{keyword_1}%\")).show(10, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed47702a-3299-45e1-d6b4-3194fa600283",
        "id": "Wx5iyTjFJP5K"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|id     |title                                                                                                                                    |keyword                                                                                                                                                         |abstract                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
            "+-------+-----------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|8600091|Improved Epoch Extraction Using Variational Mode Decomposition Based Spectral Smoothing of Zero Frequency Filtered Emotive Speech Signals|\"Smoothing methods\",\"Estimation\",\"Speech processing\",\"Frequency estimation\",\"Time-frequency analysis\",\"Resonant frequency\"                                      |This paper proposes a novel approach for epoch extraction in speech processing by utilizing variational mode decomposition based spectral smoothing of zero frequency filtered emotive speech signals. The proposed method employs smoothing methods to mitigate the noisy effect in speech signals caused by the resonant frequency. Estimation of the resonant frequency is a crucial step in speech analysis and this paper investigates the use of time-frequency analysis techniques for accurate frequency estimation. The proposed approach effectively extracts epochs from speech signals and provides improved results compared to the traditional epoch extraction methods. Overall, this study highlights the significance of reliable and accurate epoch extraction in speech processing and demonstrates the potential of the proposed method for speech analysis tasks.                                                                                                                                                                                                                                                                     |\n",
            "|8600131|Cell-Phone Identification from Recompressed Audio Recordings                                                                             |\"Mel frequency cepstral coefficient\",\"Feature extraction\",\"Audio recording\",\"Forensics\",\"Social network services\",\"Speech recognition\"                          |This paper presents a novel technique of cell-phone identification from recompressed audio recordings. The proposed approach extracts features from the audio recording by using Mel frequency cepstral coefficient. By employing feature extraction techniques, a significant improvement in the identification results has been observed. The proposed method can be used in forensic investigations to identify the source of audio recordings. The relatedness of social network services with the task of cell-phone identification from audio recordings is explored. The study indicates that social network services can be a valuable source of data for identifying the source of audio recordings. Furthermore, the paper evaluates the performance of speech recognition algorithms in the task of cell-phone identification from audio recordings. The results of the experiments prove that the proposed method outperforms the existing techniques in terms of accuracy, robustness, and efficiency. The outcome of this research can be applied in the field of forensic investigations, intelligence gathering, and security applications.|\n",
            "|8600157|Throat Microphone Speech Enhancement via Progressive Learning of Spectral Mapping Based on LSTM-RNN                                      |\"Speech enhancement\",\"Training\",\"Bandwidth\",\"Logic gates\",\"Microphones\",\"Task analysis\",\"Recurrent neural networks\"                                             |Speech enhancement is a critical task in various domains, such as telecommunications, automotive, and military. In this paper, we propose an effective approach for improving the performance of throat microphone speech enhancement using progressive learning of spectral mapping based on LSTM-RNN. Specifically, we leverage the power of recurrent neural networks to design a trainable model that can effectively capture the dependencies between consecutive frames of speech signal. By training the model on a large dataset, we can learn an optimal representation of the speech signal, which can be used to enhance the quality of signals captured by throat microphones. Our proposed method also provides an efficient way of addressing the bandwidth limitation and logic gates constraints of the throat microphone, making it suitable for real-world applications. The effectiveness of our approach is validated through extensive experiments and task analysis, demonstrating significant performance gains compared to state-of-the-art approaches.                                                                            |\n",
            "|8600190|Manner of Articulation based Split Lattices for Phoneme Recognition                                                                      |\"Lattices\",\"Decoding\",\"Hidden Markov models\",\"Speech recognition\",\"Mel frequency cepstral coefficient\",\"Feature extraction\"                                     |This paper proposes a new approach for phoneme recognition using Manner of Articulation based Split Lattices. The methodology involves feature extraction using Mel frequency cepstral coefficients and decoding using Hidden Markov models. The proposed system generates Lattices during the decoding phase which are used for the recognition of phonemes. The use of split lattices based on Manner of Articulation provides better recognition rates as compared to traditional methods. The accuracy of the proposed system was evaluated using TIMIT database and the results show a significant improvement in recognition rates. This approach has the potential to improve speech recognition systems by providing more accurate results, particularly in noisy environments.                                                                                                                                                                                                                                                                                                                                                                    |\n",
            "|8600267|Approaches to Codec Independent Speaker Identification in Voip Speech                                                                    |\"Codecs\",\"Speech coding\",\"Speech recognition\",\"Bit rate\",\"Training\",\"Mel frequency cepstral coefficient\",\"Data models\"                                          |Speaker identification has become a crucial process in the field of VoIP speech. The identification process aims to determine the identity of the speaker based on their voice characteristics, which is typically affected by speech coding, also known as codecs. Various approaches have been proposed to develop codec-independent speaker identification systems that can function well even when dealing with speech coded at different bit rates. These approaches mostly utilize speech recognition techniques, including Mel frequency cepstral coefficient (MFCC) analysis and data models. However, the effectiveness of these approaches relies heavily on the quality and quantity of the training data. Therefore, efforts are being made to develop more robust and accurate systems by improving training methods and introducing new techniques. Ultimately, the development of codec-independent speaker identification is expected to lead to the creation of more efficient and accurate VoIP speech applications that can be used across different platforms and networks.                                                            |\n",
            "|8600307|Robust Hierarchical Learning for Non-Negative Matrix Factorization With Outliers                                                         |\"Hidden Markov models\",\"Data models\",\"Linear programming\",\"Robustness\",\"Dictionaries\",\"Speech processing\",\"Convergence\"                                         |This paper presents a robust hierarchical learning approach for non-negative matrix factorization with outliers. The proposed method is based on the use of hidden Markov models and data models to identify and eliminate outliers in the factorization process. In addition, linear programming is employed to optimize the non-negative matrices and improve their robustness. The learned dictionaries can then be used for speech processing applications. The convergence of the proposed method is proven through simulations and experiments, demonstrating its effectiveness and feasibility for learning non-negative matrix factorization models in the presence of outliers.                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
            "|8600841|Treatment pillow for relieving snoring symptoms based on snore recognition                                                               |\"Hardware\",\"Sleep apnea\",\"Filter banks\",\"Speech recognition\",\"Training\",\"Feature extraction\",\"Microphones\"                                                      |This paper proposes a treatment pillow designed to alleviate snoring symptoms by utilizing snore recognition technology. The hardware of the pillow includes microphones for capturing acoustic signals, which are then filtered using filter banks to extract relevant features for speech recognition. The system is trained to recognize snoring sounds and can alert the user to change positions or take other measures to reduce snoring. This technology has the potential to improve the quality of life for sleep apnea sufferers and those around them, by allowing for a more restful sleep and subsequently reducing the risk of other sleep-related health issues. Further research and development is necessary to optimize the pillow's effectiveness and ensure its safety for long-term use.                                                                                                                                                                                                                                                                                                                                              |\n",
            "|8604332|Energy-efficient MFCC extraction architecture in mixed-signal domain for automatic speech recognition                                    |\"Mel frequency cepstral coefficient\",\"Feature extraction\",\"Computer architecture\",\"Frequency-domain analysis\",\"Energy efficiency\",\"Automatic speech recognition\"|This paper presents a novel architecture for energy-efficient Mel frequency cepstral coefficient (MFCC) extraction in the mixed-signal domain for automatic speech recognition (ASR) systems. MFCC is a popular technique for feature extraction in ASR, and this research explores an alternative approach for extracting MFCC features that reduces the energy consumption required for computation. The proposed architecture considers both the frequency-domain analysis and energy efficiency metrics to optimize the design. The design is implemented in a custom ASIC, and experimental results demonstrate that the proposed architecture is more energy-efficient than the conventional digital signal processing-based architecture while maintaining comparable ASR performance. This research provides a solution for designing energy-efficient computer architectures for feature extraction in ASR systems, which could have practical applications in portable and low-power devices.                                                                                                                                                    |\n",
            "|8604460|Spectral Masking in MFCC Calculation for Noisy Speech                                                                                    |\"Mel frequency cepstral coefficient\",\"Transform coding\",\"Estimation\",\"Harmonic analysis\",\"Psychoacoustic models\",\"Speech recognition\",\"Standards\"               |This paper explores the use of spectral masking in the calculation of Mel frequency cepstral coefficients (MFCCs) for noisy speech. MFCCs are commonly used in speech processing applications such as speech recognition, and transform coding. However, in the presence of noise, traditional MFCC calculation techniques may not be effective. This study presents a method for using spectral masking to improve estimation accuracy in noisier environments. The proposed method is based on the principles of harmonic analysis and psychoacoustic models, and is tested using a range of standard speech recognition datasets. Results show that the proposed method offers significant improvements in recognition accuracy for noisy speech, and outperforms existing techniques in many scenarios. The findings of this study demonstrate the potential benefits of spectral masking in improving the performance of speech processing applications in noisy environments.  Overall, this research offers insights into the development of new standards for speech processing in real-world settings.                                            |\n",
            "|8606825|Speech perception based on mapping speech to image by using convolution neural network                                                   |\"Speech recognition\",\"Convolution\",\"Training\",\"Spectrogram\",\"Biological neural networks\",\"Computational modeling\",\"Neurons\"                                     |This paper explores the use of a convolution neural network (CNN) for speech perception, specifically by mapping speech to image. The focus is on speech recognition and the training of a CNN using spectrogram data. The paper compares biological neural networks to computational modeling and discusses the potential for improved speech perception through machine learning. The use of neurons in the CNN is also examined, with an emphasis on the advantages and limitations of this approach. Ultimately, the paper suggests that mapping speech to image using a CNN has the potential to revolutionize speech recognition and provide more accurate and efficient results.                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
            "+-------+-----------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check keyword_2\n",
        "generation.filter(lower(col(\"keyword\")).like(f\"%{keyword_2}%\")).show(10, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a620c9ee-32ef-4731-dadb-7b2573956d85",
        "id": "-KlEDqpMJda7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|id     |title                                                                                                 |keyword                                                                                                                                      |abstract                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
            "+-------+------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|8600036|Pedestrian Detection and Attribute Analysis Program Based on CNN                                      |\"Training\",\"Image color analysis\",\"Detectors\",\"Feature extraction\",\"Semantics\",\"Monitoring\",\"Computational modeling\"                         |Pedestrian detection and attribute analysis have become important research areas due to the increasing demand for effective monitoring and surveillance systems. A CNN-based program has been developed for pedestrian detection, which involves training detectors and extracting features from color images. The program also utilizes semantics of the images for accurate detection and attribute analysis. Computational modeling techniques have been incorporated for efficient implementation of the program. The proposed program provides promising results for pedestrian detection and attribute analysis, which can be used for various surveillance applications. This research emphasizes the importance of reliable and accurate pedestrian detection for robust surveillance and monitoring systems.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
            "|8600042|Large-Scale Image Geo- Tagging Using Affective Classification                                         |\"Feature extraction\",\"Image color analysis\",\"Image classification\",\"Psychology\",\"Training\",\"Art\",\"Affective computing\"                       |This paper proposes a large-scale image geo-tagging approach using affective classification. The method first involves feature extraction from the images followed by image color analysis. Then image classification is conducted with the help of psychology-based affective computing. This approach can effectively classify images and assign them relevant geo-tags. The system's training is crucial, and it uses both art and psychology to achieve optimal accuracy. This method can have significant applications in the field of image processing, especially for social media platforms where users frequently upload images without geo-tags. The approach provides an efficient and easy-to-use solution to automatically assign location data to a large volume of untagged images.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
            "|8600068|Person Tracking and Frontal Face Capture with UAV                                                     |\"Face\",\"Feature extraction\",\"Drones\",\"Target tracking\",\"Face detection\",\"Image color analysis\"                                               |Person tracking and facial recognition with unmanned aerial vehicles (UAVs) has recently gained significant attention in research and practical applications. The ability to detect and track individuals and their facial features from a distance using drones has become a valuable asset in various fields such as security and surveillance. The main challenge for this type of technology is accurate feature extraction from the images captured by the UAVs. Face detection algorithms play a crucial role in recognizing individuals, while image color analysis methods are used to improve the accuracy of detection and tracking. This paper presents an overview of recent developments in person tracking and front-face capture with UAVs using face detection and image color analysis techniques. It aims to highlight the potential and limitations of these methods and their applications in target tracking and other areas.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
            "|8600125|Weakly Supervised Learning of Object-Part Attention Model for Fine-Grained Image Classification       |\"Image classification\",\"Convolutional neural networks\",\"Dogs\",\"Automobiles\",\"Training\",\"Visualization\",\"Testing\"                             |Fine-grained image classification has been an important task in computer vision due to its wide range of practical applications. This paper proposes a weakly supervised learning method for object-part attention model that can be used for fine-grained image classification. This method improves the performance of Convolutional Neural Networks (CNNs) by learning the important visual parts of objects in an image. The proposed model was tested on two challenging datasets of dogs and automobiles and showed significantly improved performance compared to state-of-the-art methods. The model was trained using a weakly supervised method, which means that no detailed annotations were required for training. Instead, the training relied on image-level annotations that only specified the object classes. The visualization technique was also used to analyze the object-part attention maps generated by the proposed model. Finally, extensive testing of proposed method was done, showing that it performs well on several benchmarks. Overall, the proposed weakly supervised learning of object-part attention model is a promising method for fine-grained image classification, which can be used for various applications in the field of computer vision.                                                                                                                                                                                                                                                                                                                                          |\n",
            "|8600173|Accurate Specified-Pedestrian Tracking from Unmanned Aerial Vehicles                                  |\"Target tracking\",\"Feature extraction\",\"Unmanned aerial vehicles\",\"Cameras\",\"Image color analysis\",\"Image reconstruction\",\"Legged locomotion\"|This paper proposes a method for accurate specified-pedestrian tracking from unmanned aerial vehicles (UAVs). The key challenge in the task of target tracking is feature extraction, which is hindered by the distance between the UAV and the target. To address this challenge, the authors incorporate image color analysis to enhance the feature extraction process. The proposed method also utilizes image reconstruction to generate a more detailed representation of the target, allowing for more accurate tracking. In addition, the authors explore the use of legged locomotion as another feature to aid in tracking pedestrians. Through experiments using various types of cameras, the proposed method demonstrated strong performance in accuracy and robustness. This research presents a promising approach to improve current state-of-the-art in target tracking using UAVs.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
            "|8600200|Exploiting Class Hierarchies for Large-Scale Scene Classification Using Hybrid Discriminative Approach|\"Feature extraction\",\"Visualization\",\"Databases\",\"Training\",\"Image resolution\",\"Support vector machines\",\"Semantics\"                         |In this paper, we present a hybrid discriminative approach that exploits class hierarchies for large-scale scene classification. Feature extraction plays a crucial role in this approach, and we propose a novel feature extraction method that combines both handcrafted and learned features. Additionally, visualization techniques are used to explore the learned features and provide insights into the discriminative characteristics of each feature. To facilitate training and evaluation, we also introduce a new database that contains high-resolution images of diverse scene categories. The proposed method is evaluated on this dataset and compared to conventional classifiers, demonstrating superior performance in terms of classification accuracy. Specifically, we show that our method achieves state-of-the-art performance on the dataset, outperforming other methods by a significant margin. Moreover, we also investigate the effect of image resolution on classification performance, and demonstrate that our proposed method maintains its competitiveness even with lower resolution images. Finally, the effectiveness of the proposed method is further validated by comparing it with other popular classifiers such as support vector machines, which further highlights the significant gain obtained using our hybrid discriminative approach. Through our experiments, we demonstrate the potential of exploiting class hierarchies for improving scene classification, and highlight the importance of using semantically meaningful features for large-scale image recognition tasks.|\n",
            "|8600306|A Two-Stage Attribute-Constraint Network for Video-Based Person Re-Identification                     |\"Feature extraction\",\"Task analysis\",\"Measurement\",\"Image color analysis\",\"Dynamics\",\"Learning systems\",\"Semantics\"                          |In the field of video-based person re-identification, there has been significant interest in developing more effective methods for accurately identifying individuals across different camera views. A two-stage attribute-constraint network is proposed in this paper, which involves feature extraction from the input images, task analysis to identify the relevant attributes for re-identification, and measurement of the similarity between two individuals based on their attributes. The network also utilizes image color analysis to improve the accuracy of person matching, as well as dynamics and semantics to improve the overall performance of the system. In addition, a learning system is employed to enhance the capability of the network to adapt to changes in the environment and to improve its overall robustness. The results of the experiments demonstrate that the proposed two-stage attribute-constraint network is highly effective in person re-identification tasks, and outperforms previous state-of-the-art methods in terms of accuracy and efficiency.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
            "|8600316|Deep Learning for Reliable Mobile Edge Analytics in Intelligent Transportation Systems: An Overview   |\"Cloud computing\",\"Reliability\",\"Image edge detection\",\"Computer architecture\",\"Intelligent sensors\",\"Intelligent vehicles\"                  |Intelligent Transportation Systems (ITS) are crucial for efficient and safe transportation. However, the data generated by ITS is massive, diverse, and in real-time, which poses significant challenges for data processing and analysis. Cloud computing has been used to address these challenges to a certain extent, but its reliability in terms of real-time decision-making is often questionable. This has led to the development of Mobile Edge Computing (MEC) as a solution, which enables data processing at the edge of the network. Deep Learning techniques have been applied to MEC to analyze ITS data, including image edge detection, and achieve high accuracy and fast processing speed. This paper provides an overview of the use of Deep Learning in MEC for reliable mobile edge analytics in ITS, including its integration with computer architecture, intelligent sensors, and intelligent vehicles. By leveraging the capabilities of Deep Learning, MEC offers a promising direction for intelligent transportation systems, and its future applications may revolutionize the entire transportation industry.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
            "|8600329|Texture Feature Extraction Methods: A Survey                                                          |\"Feature extraction\",\"Histograms\",\"Symmetric matrices\",\"Entropy\",\"Image segmentation\",\"Transforms\",\"Remote sensing\"                          |This paper presents a comprehensive survey of the state-of-the-art texture feature extraction methods. Texture feature extraction is an important image processing task that aims to extract informative features from images' textures for numerous applications such as image segmentation, remote sensing, and image analysis. The paper explores various techniques and approaches for feature extraction, including histogram-based methods, symmetric matrices, entropy-based methods, image segmentation-based methods, and transform-based methods. Each method is explained in detail, and its advantages and limitations are discussed. Furthermore, the paper also provides a comparative analysis of the different techniques, highlighting their strengths and weaknesses. The survey concludes by identifying the most promising approaches and discussing some of the challenges that need to be addressed in future research to improve the performance and efficiency of texture feature extraction methods. Overall, this survey serves as a valuable resource for researchers and practitioners working in the field of image processing and analysis.                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
            "|8600333|Action-Stage Emphasized Spatiotemporal VLAD for Video Action Recognition                              |\"Feature extraction\",\"Optical imaging\",\"Image coding\",\"Image segmentation\",\"Streaming media\",\"Image recognition\",\"Aggregates\"                |Video action recognition is an active area of research in computer vision. Feature extraction is an essential stage in the process of video action recognition. Optical imaging, image coding, and image segmentation play an important role in the process of feature extraction. Streaming media is a popular application area of video action recognition where the accuracy and efficiency of recognition are critical. The aggregation of local features of frames is a common method used in video action recognition. In this paper, we propose a novel spatiotemporal VLAD method that emphasizes the action-stage to improve the recognition accuracy. The proposed method aggregates the local features of frames in a video according to the action-stage they belong to, which is shown to be effective in capturing the action-stage related information. Experimental results demonstrate the superiority of the proposed method over the state-of-the-art methods in various datasets.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
            "+-------+------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check keyword_3\n",
        "generation.filter(lower(col(\"keyword\")).like(f\"%{keyword_3}%\")).show(10, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "466dfabd-4c1c-4f86-c70c-d1a4af9469f1",
        "id": "ALtjbp8wJiKS"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|id     |title                                                                                                                   |keyword                                                                                                                                                            |abstract                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
            "+-------+------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|8600401|HeadNet: An End-to-End Adaptive Relational Network for Head Detection                                                   |\"Object detection\",\"Face recognition\",\"Detectors\",\"Face\",\"Feature extraction\",\"Context modeling\"                                                                   |HeadNet is a novel end-to-end adaptive relational network designed for head detection. The detection of objects, such as faces, requires a high level of accuracy and efficiency, which is achieved through HeadNet's use of feature extraction and context modeling techniques. HeadNet uses various detectors at different levels to ensure detection accuracy and to prevent false positives. Face recognition is also a key feature of HeadNet, which enables the network to distinguish between different individuals within the same scene. The adaptive nature of HeadNet allows it to adjust to changes in the input image, such as variations in lighting and scale. The successful implementation of this approach leads to highly robust and accurate head detection, which has numerous applications across a range of different fields.                                                                                                                                                                                                                        |\n",
            "|8603045|Towards a Simplification of Medical Documents                                                                           |\"Natural language processing\",\"Classification algorithms\",\"Medical diagnostic imaging\",\"Text analysis\",\"Training\",\"Tagging\"                                        |This paper explores the potential for simplifying medical documents using natural language processing (NLP) techniques and classification algorithms to improve medical diagnosis and treatment. By analyzing medical diagnostic imaging and other text-based data, such as patient histories and lab results, NLP algorithms can identify key patterns and indicators of disease. These algorithms can be trained and tagged to improve their accuracy and effectiveness over time, allowing for increasingly accurate diagnosis and treatment. Through the use of NLP and classification algorithms, medical professionals can streamline the process of analyzing medical documents, simplifying complex information and providing more precise and accurate diagnosis and treatment plans for patients. The simplification of medical documents has the potential to revolutionize the field of medicine, improving patient outcomes and reducing costs associated with misdiagnosis and unnecessary treatments.                                                        |\n",
            "|8603586|Ligature Recognition in Urdu Caption Text using Deep Convolutional Neural Networks                                      |\"Videos\",\"Text recognition\",\"Databases\",\"Feature extraction\",\"Optical character recognition software\",\"Character recognition\",\"Image segmentation\"                 |In recent years, the recognition of ligatures in Urdu caption text has become a significant concern for researchers in the field of text recognition. This study proposes a novel method for ligature recognition in Urdu caption text. The proposed method employs deep convolutional neural networks (CNNs) to extract features from the images. A database of videos and images is used to test the proposed method, and the results have demonstrated that the proposed method outperforms existing optical character recognition software. The study also addresses the challenge of image segmentation and character recognition, which are essential to the accurate recognition of ligatures. Overall, the study suggests that deep CNNs hold significant promise for enhancing the recognition of ligatures in Urdu caption text. Further research could expand on the proposed method and explore its applicability to other languages with similar features.                                                                                                     |\n",
            "|8604033|Improving Urdu Recognition Using Character-Based Artistic Features of Nastalique Calligraphy                            |\"Character recognition\",\"Feature extraction\",\"Shape\",\"Image segmentation\",\"Writing\",\"Text recognition\",\"Microsoft Windows\"                                         |This paper proposes a novel approach to improve the recognition of Urdu characters through the use of character-based artistic features of Nastalique calligraphy. The approach involves extracting features from the shape of the characters using image segmentation techniques. These features are then used to train a character recognition system, which can read and interpret Urdu text. The proposed method is validated using a dataset of Urdu text images, and it is found that the use of artistic features can improve the accuracy of character recognition. The results are promising, and this approach has the potential to be applied in other languages and scripts as well. The technique is implemented using Microsoft Windows platform and can be useful in various applications such as writing, text recognition, and other fields.                                                                                                                                                                                                               |\n",
            "|8606831|FVI: An End-to-end Vietnamese Identification Card Detection and Recognition in Images                                   |\"Text recognition\",\"Feature extraction\",\"Optical character recognition software\",\"Image recognition\",\"Computational modeling\",\"Object detection\",\"Computer science\"|FVI is an end-to-end Vietnamese identification card detection and recognition system designed to improve the accuracy and efficiency of text recognition in images. The system utilizes advanced feature extraction and optical character recognition software to identify and extract text from images, facilitating the recognition process. The system also uses image recognition techniques to identify and locate identification cards within images, making the process of recognition more streamlined. The computational modeling and object detection techniques employed in FVI have been found to vastly improve the overall accuracy and efficiency of identification card recognition in images. This innovative system is a major advancement in the field of computer science, providing a solution to the challenges associated with text recognition and identification card recognition in image processing.                                                                                                                                             |\n",
            "|8607161|Preventing Neural Network Model Exfiltration in Machine Learning Hardware Accelerators                                  |\"Hardware\",\"Computational modeling\",\"Training\",\"Machine learning\",\"Data models\",\"Neural networks\",\"Context modeling\"                                               |This study focuses on preventing neural network model exfiltration in machine learning hardware accelerators. The hardware accelerators are widely used for computational modeling, training, and testing of various machine learning algorithms. The data models generated during the training process are highly sensitive and must be protected from unauthorized access or copy. The proposed approach uses context modeling techniques to identify and prevent exfiltration of sensitive data from the neural network model. The study highlights the importance of effective security measures to safeguard the intellectual property of machine learning algorithms and prevent cyber-attacks. The results demonstrate the effectiveness of the proposed approach in preventing exfiltration attacks on machine learning hardware accelerators, providing a viable solution to enhance the security of machine learning algorithms.                                                                                                                                  |\n",
            "|8607560|Emerging Simulation and VR for Green Innovations: A Case Study on Promoting a Zero-carbon Emission Platform in Hong Kong|\"Green products\",\"Technological innovation\",\"Solid modeling\",\"Data models\",\"Computational modeling\",\"Biological system modeling\",\"Context modeling\"                |This paper examines the application of emerging simulation and virtual reality (VR) technologies to promote green innovations, with a focus on a zero-carbon emission platform in Hong Kong. The research explores how technological innovation can be used as a tool for promoting sustainable development, particularly in the context of green products. Various modeling techniques are analyzed, including solid modeling, data models, computational modeling, biological system modeling, and context modeling, to investigate their potential for supporting effective sustainable design. Results demonstrate that simulation and VR can offer a realistic and immersive representation of green products, facilitating better understanding and decision-making around their use. It is concluded that these technologies have great potential for promoting sustainable design and innovation, and could play an important role in achieving a sustainable future.                                                                                               |\n",
            "|8607908|The Sphere of Discourse and Text                                                                                        |\"Production\",\"Linguistics\",\"Semantics\",\"Natural language processing\",\"Computational linguistics\",\"Speech\",\"Text analysis\"                                          |This paper explores the intersection between the sphere of discourse and text, with a particular focus on production, linguistics, semantics, natural language processing, computational linguistics, speech, and text analysis. Drawing upon a range of existing research in these fields, we argue that the interplay between discourse and text is a complex and nuanced one, with both individual and societal factors shaping how language is produced and interpreted. Moreover, we highlight the importance of computational methods in helping to tease out the subtleties of this relationship, drawing upon recent advances in natural language processing and text analysis to demonstrate the potential for more nuanced and sophisticated analyses of text and discourse. Ultimately, our paper argues that the continued exploration of the intersection between discourse and text is a vital area of inquiry for scholars across a range of disciplines, providing important insights into how language is produced, interpreted, and ultimately understood.|\n",
            "|8609600|LASAGNE: Locality and Structure Aware Graph Node Embedding                                                              |\"Approximation algorithms\",\"Task analysis\",\"Training\",\"Machine learning algorithms\",\"Learning systems\",\"Context modeling\",\"Graph theory\"                           |In recent years, graph embedding has become a popular research field in machine learning. Among various graph embedding methods, LASAGNE (Locality and Structure Aware Graph Node Embedding) stands out for its efficiency and accuracy. This paper introduces approximation algorithms to efficiently solve the optimization problem in LASAGNE, which enhances the training speed and accuracy of the model. Task analysis is also performed to explore the potential applications of LASAGNE in various fields. The proposed model is specifically designed for context modeling, which is a critical component in many machine learning algorithms. Moreover, LASAGNE takes into account the structural information of the graph, making it a powerful tool for graph theory research. Through extensive experiments, the proposed model has demonstrated superior performance compared with state-of-the-art graph embedding algorithms. Overall, LASAGNE provides a promising solution for learning systems that deal with complex graphs.                            |\n",
            "|8609619|Exploration of Word Embedding Model to Improve Context-Aware Recommender Systems                                        |\"Recommender systems\",\"Data mining\",\"Context modeling\",\"Feature extraction\",\"Proposals\",\"Data models\",\"Cleaning\"                                                   |Recommender systems have gained immense attention in recent years due to their ability to predict user preferences and recommend relevant items to users. However, challenges still exist in developing more accurate and effective recommender systems. This paper explores the use of word embedding models to improve context-aware recommender systems through data mining techniques, context modeling, and feature extraction. The proposed approach involves developing data models that take into account contextual factors such as time, location, and user behavior. Additionally, the cleaning of data is also a crucial step in the proposed methodology to remove irrelevant information and obtain reliable recommendations. Overall, this paper proposes a novel approach to improve context-aware recommender systems, which can enhance users’ online experience and help businesses increase their revenue.                                                                                                                                              |\n",
            "+-------+------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**polish**"
      ],
      "metadata": {
        "id": "SI-Nwi64JwLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the structure of the table\n",
        "polish.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdmNzeZTJzhp",
        "outputId": "729ba591-914d-41e8-f0db-3bf0a3e159e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- title: string (nullable = true)\n",
            " |-- keyword: string (nullable = true)\n",
            " |-- abstract: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Showcase the top five rows data\n",
        "polish.show(10, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33cZX4ctJ5Fh",
        "outputId": "42d554c2-ff41-4c1e-a3cc-f192d4072d91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|id     |title                                                                                                       |keyword                                                                                                                                   |abstract                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
            "+-------+------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|8600003|An Improved Variable-Node-Based BP Decoding Algorithm for NAND Flash Memory                                 |\"Flash memories\",\"Reliability\",\"Decoding\",\"Parity check codes\",\"Convergence\",\"Error probability\",\"Threshold voltage\"                      |To address the challenges of data reliability in NAND flash storage, a new decoding algorithm called variable-node-based belief-propagation with message pre-processing (VNBP-MP) has been proposed for binary low-density parity-check (LDPC) codes. The algorithm utilizes the unique characteristics of the NAND flash channel to perform message pre-processing (MP), which effectively prevents the spread of unreliable messages and speeds up the propagation of reliable messages. Additionally, the VNBP-MP algorithm includes a treatment for oscillating variable nodes (VNs) to further accelerate decoding convergence. \\n\\nSimulation results demonstrate that the proposed VNBP-MP algorithm has significantly improved convergence speed without compromising error-correction performance, in comparison to existing algorithms.                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
            "|8600004|Mobile Robot Location Algorithm Based on Improved Particle Filtering                                        |\"Sociology\",\"Statistics\",\"Simultaneous localization and mapping\",\"Genetic algorithms\",\"Estimation\",\"Filtering\"                            |Many techniques have been proposed to solve the simultaneous localization and mapping (SLAM) problem, and among them, the Particle Filter (PF) is considered to be one of the most effective ways. However, the PF algorithm requires a large number of samples to approximate the posterior probability density of the system, which makes the algorithm complex. Furthermore, the judgment of resampling is imperfect. In light of these challenges, this paper proposes an improved PF algorithm that introduces a population diversity factor and a genetic algorithm into the process of resampling. \\n\\nThe improved PF algorithm uses the effective sample size and the population diversity factor to determine whether to resample the particle set. When resampling is needed, the genetic algorithm is utilized to optimize the particle set. The simulation results demonstrate that the estimation accuracy of the improved algorithm is superior to that of the traditional particle filter, not only in terms of accuracy but also in efficiency.                                                                                                                                                                                                 |\n",
            "|8600008|Vertical Handoff Decision Algorithm for Heterogeneous Wireless Networks Based on Entropy and Improved TOPSIS|\"Entropy\",\"Handover\",\"Wireless networks\",\"Decision making\",\"Heterogeneous networks\"                                                       |In a future scenario with multiple wireless network coverage, the choice of vertical handoff decision algorithm will be critical to ensure session continuity, user mobility, and seamless roaming across heterogeneous wireless networks. Hence, investigating vertical handover related algorithms is essential to the success of various wireless access networks in the future. In this regard, this paper proposes an optimized algorithm that combines two multiple attribute decision making (MADM) techniques, namely the Entropy and the improved Technique for Order Preference by Similarity to an Ideal Solution (TOPSIS). The Entropy method is utilized to obtain objective weights, and the improved TOPSIS method is applied to rank the alternatives. Simulation results reveal that the proposed technique can achieve a more reasonable weight distribution and significantly reduce the number of handoffs.                                                                                                                                                                                                                                                                                                                                  |\n",
            "|8600013|Robust offline trained neural network for TDOA based sound source localization                              |\"Microphones\",\"Artificial neural networks\",\"Position measurement\",\"Training\",\"Neurons\",\"Noise measurement\",\"Microwave integrated circuits\"|Passive sound source localization using time-difference-of-arrival (TDOA) measurements is a challenging non-linear inversion problem. This paper investigates a new data-driven approach to SSL using TDOA measurements. We propose a three-layer neural network (NN) model with TDOA measurements between pairs of microphones as input and source location in the Cartesian coordinate system as output. The NN is trained to learn the non-linear mapping between TDOA measurements and source location from the training data. Interestingly, we find that the NN model can generalize well to noisy TDOA inputs, even if it is trained on noise-free TDOA data. In comparison, the traditional spherical interpolation (SI) method for localization is shown to yield inferior performance. Furthermore, we demonstrate that the NN model trained offline using simulated TDOA measurements outperforms the SI method for localizing real-life speech signals inside a simulated enclosure.                                                                                                                                                                                                                                                                 |\n",
            "|8600014|Gaussian MAC with Feedback and Strictly Causal State Information                                            |\"Encoding\",\"Transmitters\",\"Decoding\",\"Indexes\",\"Additives\",\"Estimation error\",\"AWGN channels\"                                             |We are considering a two user Gaussian multiple access channel that involves an additive Gaussian state process, where the past values of the state and received symbols are causally available to the encoders at each instant. Although the literature has solved the capacity region for the noiseless scenario without feedback, we are researching the model with both noise and feedback. Our proposed communication scheme makes use of feedback symbols and state information to enhance the achievable region. We effectively utilize the Wyner-Ziv binning on state information and Ozarow feedback scheme for the MAC. To achieve this, we use a suitable interleaving technique that results in an obtained region which is much better than the feedback capacity region without any state information.                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
            "|8600018|Efficient Detection of Phishing Attacks with Hybrid Neural Networks                                         |\"Phishing\",\"Convolution\",\"Deep learning\",\"Convolutional neural networks\",\"Uniform resource locators\",\"Computer architecture\"              |Numerous machine learning techniques and social engineering methods have been developed and utilized in the fight against phishing threats. This paper proposes a unique hybrid deep learning model to identify phishing attacks. The model comprises two components: an autoencoder (AE) and a convolutional neural network (CNN). The AE is used to reconstruct features that enhance the relationship between the features. Experimental results show that the model can detect phishing attacks with an average accuracy of 97.68%. Moreover, the model exhibits strong generalization ability and can identify phishing attacks in a timely manner.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
            "|8600029|A Social Bots Detection Model Based on Deep Learning Algorithm                                              |\"Feature extraction\",\"Metadata\",\"Detection algorithms\",\"Measurement\",\"Deep learning\",\"Social network services\",\"Data mining\"              |With the rise of the Internet, social bots have become a growing concern on social media platforms. Detecting these accounts that pose a threat to social networks has become an essential task that requires an effective detection algorithm. This paper presents a social bots detection model, called DeBD, based on a deep learning algorithm. The model is composed of three layers. The first layer is the joint content feature extraction layer, which focuses on the extraction of features from tweets' content and their relationships. The second layer is the tweet metadata temporal feature extraction layer, which uses tweet metadata as temporal information to extract user social activity temporal features through LSTM. The third layer is the feature fusion layer, which integrates the extracted joint content features with the temporal features to detect social bots. We evaluated the proposed DeBD model on three different types of social bot data sets from the real world, and the experimental results demonstrate the model's effectiveness.                                                                                                                                                                              |\n",
            "|8600031|Energy-Efficient Network Coding Scheme for Two-Way Relay Visible Light Communications                       |\"Relays\",\"Network coding\",\"Maximum likelihood estimation\",\"Radio frequency\",\"Receivers\",\"Channel estimation\",\"Visible light communication\"|In this paper, we investigate a Two-Way Relay (TWR) Visible Light Communication (VLC) system that comprises two users communicating through a relay. To improve communication efficiency, we introduce Network Coding (NC) into the VLC system and develop two energy-efficient NC-based strategies, namely Straightforward Network Coding over Finite-Alphabet Sets (SNCF) and Physical-Layer Network Coding over Finite-Alphabet Sets (PNCF). The former requires three time slots, whereas the latter requires two time slots to achieve communication. Through simulation results, we demonstrate that both the SNCF and PNCF schemes outperform the traditional four time-slot transmission scheme.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
            "|8600032|Radio Classify Generative Adversarial Networks: A Semi-supervised Method for Modulation Recognition         |\"Modulation\",\"Training\",\"Signal to noise ratio\",\"Gallium nitride\",\"Generative adversarial networks\",\"Generators\",\"Deep learning\"          |We have proposed a new framework for modulation recognition called Radio Classify Generative Adversarial Networks (RCGANs) that utilizes Generative Adversarial Network (GAN) technology in the radio machine learning domain. Through an extensive, data-driven GPU-based training process, our method learns features through self-optimization. We conducted several experiments using a synthetic radio frequency dataset, which demonstrated that our proposed method achieves higher or equivalent classification accuracy when compared to renowned deep learning and classic machine learning methods. Our technique also shows superior data utilization and robustness against noise.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
            "|8600034|Secrecy Anti-jamming Game Learning in D2D Underlay Cellular Networks with an Active Eavesdropper            |\"Jamming\",\"Games\",\"Device-to-device communication\",\"Relays\",\"Cellular networks\",\"Physical layer security\",\"Interference\"                  |In this paper, we investigate the issue of physical layer security and transmission reliability in D2D underlaying cellular networks when there is an active eavesdropper (AE) present. We propose a secrecy anti-jamming game, where the cooperation between cellular user equipment (CUE) and D2D user equipment (DUE), as well as the competition between legitimate users and AE, is formulated. In this game, DUE can either launch cooperative relaying or friendly jamming to help CUE improve its anti-eavesdropping and anti-jamming performance. CUE can offer different levels of rewards for DUE's assistance, while AE can switch between actively jamming and passively eavesdropping to maximize its destructive impact on the D2D underlaying cellular networks. We prove the existence of the pure-strategy equilibrium under perfect information and analyze the existence of the mixed-strategy equilibrium under imperfect information. We also introduce a distributed Q-learning-based algorithm that converges to the mixed-strategy equilibrium. Simulation results indicate that the proposed algorithm is convergent and that the cooperation between CUE and DUE leads to an improvement in the average utilities of legitimate users.|\n",
            "+-------+------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check keyword_1\n",
        "polish.filter(lower(col(\"keyword\")).like(f\"%{keyword_1}%\")).show(10, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxaCwEIeJ8H5",
        "outputId": "bc8bb651-35f8-4970-8b98-8f7bcb03c066"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|id     |title                                                                                                                                    |keyword                                                                                                                                                         |abstract                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
            "+-------+-----------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|8600091|Improved Epoch Extraction Using Variational Mode Decomposition Based Spectral Smoothing of Zero Frequency Filtered Emotive Speech Signals|\"Smoothing methods\",\"Estimation\",\"Speech processing\",\"Frequency estimation\",\"Time-frequency analysis\",\"Resonant frequency\"                                      |The aim of this study is to enhance the accuracy of epoch extraction from emotional speech by introducing a post-processing technique for the conventional Zero Frequency Filtering (ZFF) method, utilizing Variational Mode Decomposition (VMD) based spectral smoothing. Identifying the epochs in emotional speech signals is a challenging task due to the fast and uncontrolled variations of pitch. In the proposed method, the spectra of the ZFFS short frames are decomposed using VMD to obtain component spectra in five modes. A smoothed short-time spectra is generated by eliminating the spectra from the two higher VMD modes that contain high spectral variations. The modified ZFFS is then reconstructed utilizing the sinusoidal parameters corresponding to a single dominant frequency present in the smoothed spectra through parameter interpolation based sinusoidal synthesis using VMD. As compared to the conventional ZFF approach, the resulting re-synthesized ZFFS has reduced spurious zero crossings for emotive speech signals. This is evident from the improved epoch identification accuracy and rate for all the emotive utterances (with 7 emotions) in the German emotion speech database with simultaneous speech and electroglottographic (EGG) signal recordings. The proposed VMD based spectral post-processing approach is confirmed to be more effective than other existing ZFF based post-processing techniques for emotive speech signals in terms of the accuracy of the epoch identification when compared to the corresponding reference epochs estimated from EGG signals.|\n",
            "|8600131|Cell-Phone Identification from Recompressed Audio Recordings                                                                             |\"Mel frequency cepstral coefficient\",\"Feature extraction\",\"Audio recording\",\"Forensics\",\"Social network services\",\"Speech recognition\"                          |Audio forensics has numerous applications that can benefit from the ability to classify audio recordings based on the device they originated from, especially on social media platforms where huge amounts of data are posted daily. In this regard, a recent paper proposes to leverage passive signatures associated with recording devices extracted from recorded audio, in identifying the source cell-phone of the recorded audio. This approach uses device-specific information present in both low and high-frequency regions of the audio recordings, even in the absence of any extrinsic security mechanism like digital watermarking. The proposed system achieves a closed set accuracy of 97.2% on the only publicly available dataset in this field, MOBIPHONE, matching the state-of-the-art accuracy reported for this dataset. Furthermore, the proposed methodology outperforms existing methods by 4% in average accuracy on audio recordings which have undergone double compression, which frequently occurs when a recording is shared on social media.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
            "|8600157|Throat Microphone Speech Enhancement via Progressive Learning of Spectral Mapping Based on LSTM-RNN                                      |\"Speech enhancement\",\"Training\",\"Bandwidth\",\"Logic gates\",\"Microphones\",\"Task analysis\",\"Recurrent neural networks\"                                             |In this paper, a new progressive spectral mapping learning algorithm is proposed for throat microphone (TM) speech enhancement. Rather than using full-band spectra mapping algorithms, this new algorithm splits the mapping into two tasks: voice conversion and artificial bandwidth extension. Additionally, a Long short-term memory recurrent neural network (LSTM-RNN) is utilized as the mapping model. The objective evaluation results demonstrate a significant improvement in TM speech quality compared to conventional full-band spectra mapping framework and DNN-based mapping model.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
            "|8600190|Manner of Articulation based Split Lattices for Phoneme Recognition                                                                      |\"Lattices\",\"Decoding\",\"Hidden Markov models\",\"Speech recognition\",\"Mel frequency cepstral coefficient\",\"Feature extraction\"                                     |Phoneme lattices have been shown to be a reliable method for encoding alternative decoding hypotheses from a speech recognition system. However, the search space of the decoder can become too large if the optimal phoneme sequence is produced by tracing all the phoneme identities in the lattice. This can lead to false substitutions or insertion errors in the final phoneme sequence. \\n\\nTo solve this problem, a new approach called split lattice structures is introduced in this paper. The split lattice structures are generated by splitting the speech frames based on the manner of articulation, which is detected using the spectral flatness measure (SFM). The sonorants (including vowels, semivowels, and nasals) and non-sonorants (including fricatives, stops, and closures) are separated into different split lattices. \\n\\nThe split lattices are modified based on the manner of articulation in each split to eliminate irrelevant phoneme identities in the lattice. For example, non-sonorant phoneme identities are excluded from the sonorant lattice, reducing false substitutions or insertion errors. \\n\\nThe proposed split lattice structure based on sonority detection demonstrated a 0.9% decrease in phone error rates compared to conventional decoding using state-of-the-art Deep Neural Networks (DNN) when evaluated on the core TIMIT test corpus.                                                                                                                                                                                                                            |\n",
            "|8600267|Approaches to Codec Independent Speaker Identification in Voip Speech                                                                    |\"Codecs\",\"Speech coding\",\"Speech recognition\",\"Bit rate\",\"Training\",\"Mel frequency cepstral coefficient\",\"Data models\"                                          |The performance of Automatic Speaker Identification (ASI) systems on Voice over Internet Protocol (VoIP) speech is highly dependent on the type of codec used in the communication. Since the codec used varies based on the service provider of the user, there is a necessity to have codec-independent ASI systems that can accurately identify the speaker. \\n\\nTo achieve this goal, three different modeling approaches based on the Unified Background Model (UBM)-Gaussian Mixture Model (GMM) framework and the i-vector framework have been proposed. These modeling approaches enable the identification of the speaker independent of the codec used. Further, these frameworks are also evaluated for mismatch conditions with respect to the codec used for training and testing. \\n\\nThe proposed approaches have been tested on VoIP speech data obtained from four different codecs with different bit rates, along with uncoded speech. The results reveal that these approaches lead to improved identification accuracy, especially in mismatched conditions. Hence, these proposed methods show significant potential for enabling more efficient and robust ASI systems for VoIP applications.                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
            "|8600307|Robust Hierarchical Learning for Non-Negative Matrix Factorization With Outliers                                                         |\"Hidden Markov models\",\"Data models\",\"Linear programming\",\"Robustness\",\"Dictionaries\",\"Speech processing\",\"Convergence\"                                         |Desirable properties of extensions to Non-negative Matrix Factorization (NMF) include robustness in the presence of noise and outliers, ease of implementation, guaranteed convergence, automatic balancing of data approximation and model simplicity, and the ability to model sequential structures of time-series signals. Unfortunately, these desirable properties are rarely all present in a single NMF extension. This paper proposes a novel approach that possesses all of these desirable properties by extending the Automatic Relevance Determination (ARD) framework in NMF by Tan and Févotte. Using an objective function derived from maximum a posteriori estimation of a Bayesian model, the authors develop majorization-minimization algorithms that effectively determine the correct model order, despite the impact of noise and outliers. Additionally, the proposed algorithms are rigorously analyzed for convergence. The authors also incorporate convolutive bases in the model to capture the temporal continuity of data. The efficiency and robustness of this approach are demonstrated through experiments on both synthetic and real-world datasets.                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
            "|8600841|Treatment pillow for relieving snoring symptoms based on snore recognition                                                               |\"Hardware\",\"Sleep apnea\",\"Filter banks\",\"Speech recognition\",\"Training\",\"Feature extraction\",\"Microphones\"                                                      |The main objective of this paper is to develop an embedded platform that uses the TMS320VC5509A processor for snore recognition and controlling pillow height to alleviate associated symptoms. This entire embedded hardware platform collects sound through the microphone module while also storing, processing and interacting with data through other peripherals. After several pre-processing operations, a dual threshold detection algorithm utilizing short-time energy and short-time zero crossing rate is employed for endpoint recognition. Additionally, MFCC is employed for feature extraction while KNN algorithm is selected for recognition. Experimental results demonstrate that the system operates effectively and the design is robust. Furthermore, the platform achieves high accuracy in snore analysis and recognition.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
            "|8604332|Energy-efficient MFCC extraction architecture in mixed-signal domain for automatic speech recognition                                    |\"Mel frequency cepstral coefficient\",\"Feature extraction\",\"Computer architecture\",\"Frequency-domain analysis\",\"Energy efficiency\",\"Automatic speech recognition\"|This paper presents a new processing architecture for automatic speech recognition that focuses on the efficient extraction of Mel-Frequency Cepstrum Coefficients (MFCC). The architecture is inspired by the human ear and adopts energy-efficient analog-domain information processing to replace the energy-intensive Fourier Transform typically used in digital-domain processing. In addition, this proposed architecture extracts acoustic features in mixed-signal domains, which helps to reduce the cost of Analog-to-Digital Converters (ADCs) and computational complexity. \\n\\nCircuit-level simulation based on 180nm CMOS technology shows that the proposed architecture can achieve an energy consumption of 2.4 nJ/frame and a processing speed of 45.79 μs/frame. By performing this processing in analog and mixed-signal domains, the new architecture achieves a significant 97.2% energy savings and a 6.4× speedup compared to the state of the art. Speech recognition simulation results indicate that the proposed MFCC features are highly effective, with a classification accuracy of 99%.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
            "|8604460|Spectral Masking in MFCC Calculation for Noisy Speech                                                                                    |\"Mel frequency cepstral coefficient\",\"Transform coding\",\"Estimation\",\"Harmonic analysis\",\"Psychoacoustic models\",\"Speech recognition\",\"Standards\"               |To enhance the noise immunity of the commonly used mel-frequency cepstral coefficients (MFCC) for voice signal parametrization, a possible solution is to incorporate a psychoacoustic model of frequency masking. Furthermore, by considering the formation mechanism of formant regions in the voice signal spectrum, the spectral samples that correspond to multiple harmonics of the fundamental tone could also be modified. The effectiveness of this modified algorithm was tested using a single word recognition system that is adapted for MFCC voice signal parametrization only. The results showed that this proposed additional voice signal transformation has a positive impact on the parametrization algorithm.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
            "|8606825|Speech perception based on mapping speech to image by using convolution neural network                                                   |\"Speech recognition\",\"Convolution\",\"Training\",\"Spectrogram\",\"Biological neural networks\",\"Computational modeling\",\"Neurons\"                                     |Although there has been significant research conducted in the field of speech perception for over six decades, there are still substantial discoveries to be made. Previous studies have predominantly focused on speech recognition, overlooking other components of speech perception. In this paper, we present a novel approach to understanding speech perception as a process of learning to associate speech with other environmental cues. By viewing speech perception in this new light, the problem can be reconceptualized as a relationship learning challenge. Our proposed solution involves employing a convolution neural network to map speech signals to images.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
            "+-------+-----------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check keyword_2\n",
        "polish.filter(lower(col(\"keyword\")).like(f\"%{keyword_2}%\")).show(10, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9RGNIqcJ_WV",
        "outputId": "e3f132b4-5b2b-4437-b809-8ae376368b8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|id     |title                                                                                                 |keyword                                                                                                                                      |abstract                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
            "+-------+------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|8600036|Pedestrian Detection and Attribute Analysis Program Based on CNN                                      |\"Training\",\"Image color analysis\",\"Detectors\",\"Feature extraction\",\"Semantics\",\"Monitoring\",\"Computational modeling\"                         |In recent years, deep learning object detectors such as Fast/Faster R-CNN, SSD, R-FCN, and Mask R-CNN have exhibited significant performance for general object detection, except for pedestrians. The Faster R-CNN's Region Proposal Network (RPN) functions effectively but lacks adaptability. Therefore, we suggest an adaptive real-time pedestrian detection and attribute identification scheme based on Caffe. The first contribution is the Adaptive Threshold Adjustment (ATA) algorithm for intelligent monitoring, which adjusts the threshold using pedestrian movement information. Moreover, to overcome the time-consuming defect, we analyzed the influences of the number of layers, the size of convolution kernels, and the number of feature maps to reduce redundant computation, while maintaining satisfactory performance. By optimizing the neutral network structure, selecting model parameters, and data augmentation, we obtained a stable and well-performed model with fast detection rates and high accuracy. Furthermore, pedestrian information can be identified in our program, providing better service in security monitoring, intelligent robots, and other fields. Extensive experimental results demonstrate that even in complex and athletic scenarios, our method can make quality and speed improvements over current state-of-the-art techniques.                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
            "|8600042|Large-Scale Image Geo- Tagging Using Affective Classification                                         |\"Feature extraction\",\"Image color analysis\",\"Image classification\",\"Psychology\",\"Training\",\"Art\",\"Affective computing\"                       |Images have a powerful impact on human emotions and decision-making by conveying a wealth of information in a single frame. With the development of \"Affective Computing\", machines can also be designed with emotional intelligence, enabling them to make decisions based on emotions. This emotional aspect of machine learning has been applied to various fields such as E-Health and E-learning. This study focuses on the use of emotional aspects in machines for Geo-tagging of images. The proposed solution employs a hybrid approach to Affective Image Classification by combining Elements-of-Art based emotional features (EAEF) and Principles-of-Art based emotional features (PAEF). The experiment involves testing each set of features individually and then combining them to form a Hybrid feature vector. Results show that the hybrid approach significantly outperforms either individual approach. Images for this project were obtained from the Yahoo Flickr Creative Commons 100 Million (YFCC100M) dataset, which includes millions of images with corresponding coordinates and are free to use.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
            "|8600068|Person Tracking and Frontal Face Capture with UAV                                                     |\"Face\",\"Feature extraction\",\"Drones\",\"Target tracking\",\"Face detection\",\"Image color analysis\"                                               |This paper presents a novel approach to track a walking person and automatically capture a frontal photo using an unmanned aerial vehicle (UAV). The proposed method consists of three main parts, namely person detection and recognition, face detection and feature points localization, and vision-based UAV control. The person tracking module employs the YOLOv3 deep neural network for detecting the target person, and utilizes the Locality-constrained Linear Coding (LLC) algorithm for matching the target person. The facial recognition module relies on the Multi-task Cascaded Convolutional Neural Networks (MTCNN) for detecting frontal faces. The UAV control is based on the information gathered from the two previous modules, which allows the UAV to fly around the target person and obtain their frontal face image. The results of outdoor experiments using a Parrot Bebop2 drone show the effectiveness and practicality of this approach.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
            "|8600125|Weakly Supervised Learning of Object-Part Attention Model for Fine-Grained Image Classification       |\"Image classification\",\"Convolutional neural networks\",\"Dogs\",\"Automobiles\",\"Training\",\"Visualization\",\"Testing\"                             |Fine-grained classification is a challenging task, primarily due to the small inter-class variance and large intra-class distance between fine-grained categories. The critical factor in solving this problem is to identify the discriminative part of the image accurately. To address this issue, we propose a weakly supervised method that only requires image-level labels for fine-grained classification.\\n\\nOur model employs a convolutional neural network (CNN), which can locate the discriminative region through attention and automatically focus on subtle features by zooming in on the discriminative region and feeding it to the next CNN. We also incorporate a Squeeze and Excitation (SE) module for channel-wise attention to enhance the model's discriminative ability. Additionally, a spatial constrain loss is utilized to maintain the diversity of the located part.\\n\\nTo evaluate the performance of our model, we conducted experiments on three datasets: CUB-2011-200, Stanford Dogs, and Stanford Cars. The experimental results demonstrate the effectiveness of our proposed method compared to other methods.\\n\\nIn summary, our weakly supervised method provides a promising solution for fine-grained classification tasks by accurately locating the discriminative part of the image and enhancing the model's ability to focus on subtler features.                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
            "|8600173|Accurate Specified-Pedestrian Tracking from Unmanned Aerial Vehicles                                  |\"Target tracking\",\"Feature extraction\",\"Unmanned aerial vehicles\",\"Cameras\",\"Image color analysis\",\"Image reconstruction\",\"Legged locomotion\"|The field of Unmanned Aerial Vehicles (UAV) has recently seen a surge in the use of accurate target tracking. This paper focuses on the real-time detection and tracking of a walking pedestrian from a moving platform, amidst many interferences. To achieve this, we propose a scheme that employs CNN model (YOLO-V2) to detect pedestrian and matches the walking pedestrian with both postprocessing and feature queue, as well as Locality constrained Linear Coding algorithm. The ground station then receives and analyzes the video stream from the Parrot and sends back commands to control the motion of the UAV. At the onset of the tracking process, the UAV hovers while one pedestrian is selected as the designated target. We rely solely on visual information obtained via a front camera without assistant sensors. For the experiment, we utilized a Parrot Bebop 2, which allowed us to conduct outdoor experiments. The experimental results validate the effectiveness of our proposed solution.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
            "|8600200|Exploiting Class Hierarchies for Large-Scale Scene Classification Using Hybrid Discriminative Approach|\"Feature extraction\",\"Visualization\",\"Databases\",\"Training\",\"Image resolution\",\"Support vector machines\",\"Semantics\"                         |Humans can effortlessly perceive a vast amount of information while observing a scene. In contrast, scene recognition is a challenging problem for computers due to the variability, ambiguity, and diverse illumination and scale conditions of scene images. Therefore, scene classification is an essential task that provides contextual information to guide other processes, such as browsing, content-based image retrieval, and object recognition. To better evaluate the proposed solution, a baseline model based on the traditional bag of words model is established. In addition, a model based on the idea of fine to coarse category mappings is proposed, which combines information with the fusion of feature descriptors to yield a single feature representation. This approach exploits the hierarchical relationships among the scene categories, leading to enhanced performance. Various evaluation metrics validate the effectiveness of the proposed method, which outperforms the given baseline and several state-of-the-art methods. Moreover, the proposed framework achieves an appropriate balance between the time and accuracy of the model.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
            "|8600306|A Two-Stage Attribute-Constraint Network for Video-Based Person Re-Identification                     |\"Feature extraction\",\"Task analysis\",\"Measurement\",\"Image color analysis\",\"Dynamics\",\"Learning systems\",\"Semantics\"                          |Person re-identification has gained popularity in various fields, such as security, criminal investigation, and video analysis. The purpose of this paper is to develop a two-stage attribute-constraint network known as the TSAC-Net to learn a discriminate and robust spatial-temporal representation for video-based person re-identification. The use of pedestrian attributes containing high-level information can aid in re-identification tasks and is resilient to visual variations. In this paper, we manually annotated three video-based person re-identification datasets with four static appearance attributes and one dynamic appearance attribute, where each attribute serves as a constraint added to the deep network. In the TSAC-Net's first stage, we solved the re-identification problem by using a classification approach and adopted a multi-attribute classification loss to train the CNN model. In the second stage, two LSTM networks were trained under identity and dynamic appearance attribute constraints. With this approach, the two-stage network offers a spatial-temporal feature extractor for pedestrian sequences in videos. During the testing phase, inputting a sequence of images to the proposed TSAC-Net creates a spatial-temporal representation. Our method demonstrates an improvement in performance attained with attribute usage on several challenging person re-identification datasets such as PRID2011, iLIDS-VID, MARS, and VIPeR. Furthermore, our extensive experimentation has shown that our approach's state-of-the-art performance achieves results for three video-based benchmark datasets.                                                                                                                                                                                   |\n",
            "|8600316|Deep Learning for Reliable Mobile Edge Analytics in Intelligent Transportation Systems: An Overview   |\"Cloud computing\",\"Reliability\",\"Image edge detection\",\"Computer architecture\",\"Intelligent sensors\",\"Intelligent vehicles\"                  |Intelligent transportation systems (ITS) will play a critical role in the development of smart cities. However, to unlock the true potential of ITS, there is a need for real-time ultralow latency and reliable analytics solutions that can combine data from a diverse range of sources. Conventional cloud-centric data processing techniques are not suitable for this task due to their high communication and computing latency. This calls for the development of edge-centric solutions that are customized to the unique ITS environment. \\n\\nIn this article, we introduce an edge analytics architecture for ITS that processes data at the vehicle or roadside smart sensor level, thus overcoming the reliability and latency challenges inherent in ITS. This distributed edge computing architecture leverages deep-learning techniques to provide reliable mobile sensing. We explore the various challenges facing mobile edge analytics in ITS, including data heterogeneity, autonomous control, vehicular platoon control, and cyberphysical security. We then discuss various deep-learning solutions that can help overcome these challenges. \\n\\nThese deep-learning solutions endow ITS devices with powerful computer vision and signal processing functions, enabling the ITS edge analytics to overcome the challenges. Initial results show that this combined approach provides a transportation environment that is secure, reliable, and truly smart. \\n\\nIn conclusion, the future of ITS will be integrated with smart cities, and edge analytics architecture powered by deep-learning techniques will be a critical component. This architecture will provide reliable, secure, and ultralow latency data analytics solutions for the ITS environment, thus benefiting drivers, passengers, and city planners alike.|\n",
            "|8600329|Texture Feature Extraction Methods: A Survey                                                          |\"Feature extraction\",\"Histograms\",\"Symmetric matrices\",\"Entropy\",\"Image segmentation\",\"Transforms\",\"Remote sensing\"                          |Texture analysis is a technique that finds applications in diverse fields such as remote sensing, biomedical imaging, pattern recognition, and image synthesis. The first step in all of these image processing techniques is to extract meaningful features that describe texture properties from raw images. Over the years, several feature extraction methods have been proposed, each with its advantages and limitations.\\n\\nThis paper presents a comprehensive survey of texture feature extraction methods and categorizes them into seven classes: statistical approaches, structural approaches, transform-based approaches, model-based approaches, graph-based approaches, learning-based approaches, and entropy-based approaches. For each category, the concept, advantages, and drawbacks are discussed along with examples of their applications.\\n\\nThe survey helps identify two categories of methods that require further study due to their promising performance. By highlighting the strengths and weaknesses of each method, this survey serves as a valuable resource for researchers in the field of texture analysis.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
            "|8600333|Action-Stage Emphasized Spatiotemporal VLAD for Video Action Recognition                              |\"Feature extraction\",\"Optical imaging\",\"Image coding\",\"Image segmentation\",\"Streaming media\",\"Image recognition\",\"Aggregates\"                |Although convolutional neural networks (CNNs) are highly effective for image recognition, they have not yet produced impressive results for action recognition in videos. Their inability to model long-range temporal structures, particularly those involving individual action stages, is a significant limiting factor for human action recognition. To address this limitation, we present a novel approach called ActionS-ST-VLAD that emphasizes action stages, aggregates informative deep features, and segments videos based on adaptive video feature segmentation and adaptive segment feature sampling (AVFS-ASFS). Our ActionS-ST-VLAD encoding method uses AVFS-ASFS to automatically split deep features into segments, with features in each segment corresponding to a temporally coherent action stage. We then use a flow-guided warping technique to detect and discard redundant feature maps and aggregate the informative ones using an exploited similarity weight. Additionally, we introduce an RGBF modality to capture motion salient regions in RGB images corresponding to action activity. Extensive experiments on the HMDB51, UCF101, Kinetics, and ActivityNet benchmarks demonstrate that our method effectively pools useful deep features spatiotemporally, resulting in state-of-the-art performance for video-based action recognition.                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
            "+-------+------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check keyword_3\n",
        "polish.filter(lower(col(\"keyword\")).like(f\"%{keyword_3}%\")).show(10, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgsoPi3OKC-s",
        "outputId": "01848075-ca06-4264-d3db-90370748b9bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|id     |title                                                                                                                   |keyword                                                                                                                                                            |abstract                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
            "+-------+------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|8600401|HeadNet: An End-to-End Adaptive Relational Network for Head Detection                                                   |\"Object detection\",\"Face recognition\",\"Detectors\",\"Face\",\"Feature extraction\",\"Context modeling\"                                                                   |Head detection is a crucial task in identifying individuals from visual data. However, due to the difficulty of building local and global information under unconstrained pose and orientation conditions, existing methods have had limitations in effectively detecting heads. To address these issues, this study proposes an adaptive relational network capable of capturing context information. The fundamental contextual features, such as global shape priors and local adjacent relationships, can be quantified by visual operators. The authors propose a two-step search algorithm to quantify intergroup conflict, while also introducing a structured feature module to capture local intraindividual stability. Finally, the global priors and local relation are integrated into a single-stage head detector. The study shows that the proposed method achieves state-of-the-art results on two challenging datasets, HollywoodHeads and Brainwash. An extensive ablation analysis further confirms the efficacy of the approach.                                                                                                                                                                                                                                         |\n",
            "|8603045|Towards a Simplification of Medical Documents                                                                           |\"Natural language processing\",\"Classification algorithms\",\"Medical diagnostic imaging\",\"Text analysis\",\"Training\",\"Tagging\"                                        |In this paper, we present a solution that aims to make reading and comprehending medical documents easier by automatically simplifying complex medical terms found in web pages. Our proposed approach combines natural language processing and heuristics to identify these terms and calculate the probability of them being medical and complex. We then use a customized dictionary to simplify these terms, and leverage the Microsoft Bing cognitive API to retrieve related images and videos, which are presented to the users for a better understanding of the document being read.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
            "|8603586|Ligature Recognition in Urdu Caption Text using Deep Convolutional Neural Networks                                      |\"Videos\",\"Text recognition\",\"Databases\",\"Feature extraction\",\"Optical character recognition software\",\"Character recognition\",\"Image segmentation\"                 |Textual content within videos contains a plethora of information that can be used for semantic indexing, retrieval, and development of video analytics solutions. To achieve this, a textual content-based video retrieval system needs to detect and localize text, followed by recognition, which is what this paper focuses on. Specifically, this research presents a caption text recognition system, which targets Urdu text and relies on a holistic approach using ligatures as units of recognition. To do this, the technique employs data-driven feature extraction techniques using a number of pre-trained deep convolution neural networks, which are used as feature extractors and fine-tuned on the ligature dataset under study. The results showed high ligature recognition rates, demonstrating the system's effectiveness.                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
            "|8604033|Improving Urdu Recognition Using Character-Based Artistic Features of Nastalique Calligraphy                            |\"Character recognition\",\"Feature extraction\",\"Shape\",\"Image segmentation\",\"Writing\",\"Text recognition\",\"Microsoft Windows\"                                         |State-of-the-art approaches for Urdu character recognition using the Nastalique writing style utilize features and character label sequences for classification and recognition. Connected strokes of ligatures in Arabic-like cursive script are conventionally processed as a sequence of characters. However, this approach disregards the fact that ligature images have a sequence of pairs of characters and joiners, rather than individual characters. This paper presents an implicit recognition technique that extracts artistic features of characters and joiners from detailed analysis of Nastalique calligraphy. Tests were conducted on two datasets, with accuracies of 95.58% and 98.37%. Results indicate superior performance compared to existing hidden Markov models and deep learning-based recognition techniques.                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
            "|8606831|FVI: An End-to-end Vietnamese Identification Card Detection and Recognition in Images                                   |\"Text recognition\",\"Feature extraction\",\"Optical character recognition software\",\"Image recognition\",\"Computational modeling\",\"Object detection\",\"Computer science\"|The need to digitize all old, outdated forms of identification cards and register books has become a pressing concern, given their importance in practical applications for sales and financial services. To tackle this issue, we have developed an End-to-end Identification Card Recognition system that enables fast detection, text recognition, and vital information extraction from ID cards. This system not only comprises efficient modeling techniques for text detection and recognition, but robust architecture design of FVI that is already deployed in numerous organizations.\\n\\nThrough extensive evaluations, we have verified that our efficient system is well-suited for large-scale detection and recognition of constrained forms. With this digitalization, organizations can benefit from quicker and more accurate processing of vital data, which can lead to improved services and overall performance in the future.                                                                                                                                                                                                                                                                                                                                         |\n",
            "|8607161|Preventing Neural Network Model Exfiltration in Machine Learning Hardware Accelerators                                  |\"Hardware\",\"Computational modeling\",\"Training\",\"Machine learning\",\"Data models\",\"Neural networks\",\"Context modeling\"                                               |Machine learning (ML) models are extensively used across various domains and are often trained using private datasets that may be very expensive to collect or highly sensitive. The ML models may be exposed through online APIs or used in hardware devices deployed in the field or used by the end-users. This exposes these ML models to the risk of theft by malicious actors who may use them as a proxy for gathering datasets. Although API-based model exfiltration has been studied, the theft and protection of machine learning models on hardware devices have not been explored yet. Addressing this critical aspect of the design and deployment of ML models, we propose power-efficient obfuscation as an alternative to encryption and timing side-channel countermeasures as effective ways to secure these models from theft by attackers who may acquire the model or the model architecture through memory probing, side-channels, or crafted input attacks.                                                                                                                                                                                                                                                                                                          |\n",
            "|8607560|Emerging Simulation and VR for Green Innovations: A Case Study on Promoting a Zero-carbon Emission Platform in Hong Kong|\"Green products\",\"Technological innovation\",\"Solid modeling\",\"Data models\",\"Computational modeling\",\"Biological system modeling\",\"Context modeling\"                |Researchers are interested in the application of various technologies to drive social innovations. While previous articles have discussed the potential role of different tasks in a social and green innovation context, there are limited studies on the development of process models to govern technological applications in these contexts. This gap in research may impede the capacity of these applications to deliver benefits to the general public. To address this challenge, we conducted a case study investigating the use of virtual reality (VR) and simulation to promote a green deck project in Hong Kong. Drawing on our findings, we have developed a four-step process model for leveraging simulation and VR to drive green innovations. These steps include planning, modeling, verifying, and application. Implementing this process model can guide practitioners in their use of simulation and VR to promote and support green initiatives.                                                                                                                                                                                                                                                                                                                     |\n",
            "|8607908|The Sphere of Discourse and Text                                                                                        |\"Production\",\"Linguistics\",\"Semantics\",\"Natural language processing\",\"Computational linguistics\",\"Speech\",\"Text analysis\"                                          |This chapter delves into the fundamental concepts of discourse analysis and examines the associated terminology. The central theme of discourse analysis is coherence, encompassing all the extra-sentential aspects of a text. Discourse comprises of utterances, i.e., sentences employed within a precise context with a specific communicative aim, displaying coherence as its essential feature. The interpretation of an utterance is not solely reliant on the overt components present in it. The present chapter elucidates the construction of a tree structure for discourse - this level of analysis is equivalent to the syntax of discourse. The interpretation of this syntactic structure necessitates a semantic framework, which is where Discourse Representation Theory (DRT) becomes relevant. In modern semantics, there is a new standpoint that focuses on discourse as the fundamental unit that must possess a truth value instead of the sentence.                                                                                                                                                                                                                                                                                                               |\n",
            "|8609600|LASAGNE: Locality and Structure Aware Graph Node Embedding                                                              |\"Approximation algorithms\",\"Task analysis\",\"Training\",\"Machine learning algorithms\",\"Learning systems\",\"Context modeling\",\"Graph theory\"                           |In this work, we introduce LASAGNE, a method for unsupervised learning of graph node embeddings that takes into account locality and structure awareness. Existing approaches based on random walks have been shown to be highly dependent on the structural properties of the graph, such as its size, the shape of the Network Community Profile (NCP), whether it is expander-like, and the characteristics of the classes of interest. For larger graphs with flat NCPs that are highly expander-like, the quality of the vector representations obtained through these methods is often lower due to the rapid expansion of random walks touching dissimilar nodes.\\n\\nLASAGNE circumvents this issue by utilizing localized Approximate Personalized PageRank stationary distributions to incorporate more precise local information into node embeddings, rather than relying on global random walks or fixed hop distances. This methodology results in more meaningful and useful vector representations for nodes in poorly-structured graphs. Our experiments show that LASAGNE produces significant improvements in downstream multi-label classification for larger graphs with flat NCPs, while maintaining comparable performance for smaller graphs with upward-sloping NCPs.|\n",
            "|8609619|Exploration of Word Embedding Model to Improve Context-Aware Recommender Systems                                        |\"Recommender systems\",\"Data mining\",\"Context modeling\",\"Feature extraction\",\"Proposals\",\"Data models\",\"Cleaning\"                                                   |Recommender systems are designed to assist users by recommending items that are likely to be of interest to them. Traditionally, these systems rely primarily on user and item information to generate recommendations. However, more recently, contextual information has been incorporated into these systems, leading to improved accuracy. This paper proposes a context-aware recommender approach that utilizes contextual information extracted from textual reviews through a word embedding based model. The proposed method also considers two ways of contextualizing the recommendations, the \"Context of Reviews\" and the \"Context of Items\". The proposed approach is evaluated using the Yelp dataset (RecSysChallenge 2013), along with three baseline models and four context-aware recommender systems. Overall, the proposed approach outperforms the baseline models, particularly when considering the \"Context of Items\". The results are quite promising, and suggest future lines of research in this area.                                                                                                                                                                                                                                                          |\n",
            "+-------+------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**human**"
      ],
      "metadata": {
        "id": "L1c3SwFbKKLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the structure of the table\n",
        "human.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSnX_xO9KQyB",
        "outputId": "3543edd5-050e-477d-bc19-bd12cdc6d208"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- title: string (nullable = true)\n",
            " |-- keyword: string (nullable = true)\n",
            " |-- abstract: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Showcase the top five rows data\n",
        "human.show(10, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eoy8w1twKUQy",
        "outputId": "f7ad552a-c634-4f42-b668-5c0782f308b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|id     |title                                                                                                       |keyword                                                                                                                                   |abstract                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
            "+-------+------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|8600003|An Improved Variable-Node-Based BP Decoding Algorithm for NAND Flash Memory                                 |\"Flash memories\",\"Reliability\",\"Decoding\",\"Parity check codes\",\"Convergence\",\"Error probability\",\"Threshold voltage\"                      |To solve the problems of the data reliability for NAND flash storages, a variable-node-based belief-propagation with message pre-processing (VNBP-MP) decoding algorithm for binary low-density parity-check (LDPC) codes is proposed. The major feature is that, by making use of the characteristics of the NAND flash channel, the proposed algorithm performs the message pre-processing (MP) scheme to effectively prevent the propagation of unreliable messages and speed up the propagation of reliable messages. To further speed up the decoding convergence, the treatment for oscillating variable nodes (VNs) is considered after the MP scheme being employed. Simulation results show that the proposed VNBP-MP algorithm has a noticeable improvement in convergence speed without compromising the error-correction performance, compared with the existing algorithms.                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
            "|8600004|Mobile Robot Location Algorithm Based on Improved Particle Filtering                                        |\"Sociology\",\"Statistics\",\"Simultaneous localization and mapping\",\"Genetic algorithms\",\"Estimation\",\"Filtering\"                            |To solve the simultaneous localization and mapping (SLAM) problem, many techniques have been proposed, and the Particle Filter (PF) is one of effective ways. However, the PF algorithm needs a large number of samples to approximate the posterior probability density of the system, which makes the algorithm complex. What's more, the judgment of resampling is imperfect. Based on this, an improved PF algorithm which introducing population diversity factor and genetic algorithm into the process of re-sampling is proposed in this paper. The effective sample size and the population diversity factor are used to determine whether to re-sampling. When re-sampling is needed, the genetic algorithm is used to optimize the particle set. The simulation result shows that estimation accuracy of the improved algorithm is better than that of traditional particles filter, not only in accuracy, but also in efficiency.                                                                                                                                                                                                                                                                                                                                                          |\n",
            "|8600008|Vertical Handoff Decision Algorithm for Heterogeneous Wireless Networks Based on Entropy and Improved TOPSIS|\"Entropy\",\"Handover\",\"Wireless networks\",\"Decision making\",\"Heterogeneous networks\"                                                       |In the future scenario of multiple wireless network coverage, the choice of vertical handoff decision algorithm will directly affect the continuity of the session, the mobility of the user, and seamless roaming under heterogeneous wireless networks. Therefore, the study of vertical handover related algorithms is the key to the success of various wireless access networks in the future. This paper proposes an optimized algorithm which combines two multiple attribute decision making (MADM) methods, the Entropy and the improved Technique for Order Preference by Similarity to an Ideal Solution (TOPSIS). The Entropy method is applied to obtain objective weights and the improved TOPSIS method is used to rank the alternatives. The simulation results show that the proposed technique can make the distribution of weights more reasonable, and effectively reduce the number of handoffs.                                                                                                                                                                                                                                                                                                                                                                                  |\n",
            "|8600013|Robust offline trained neural network for TDOA based sound source localization                              |\"Microphones\",\"Artificial neural networks\",\"Position measurement\",\"Training\",\"Neurons\",\"Noise measurement\",\"Microwave integrated circuits\"|Passive sound source localization (SSL) using time-difference-of-arrival (TDOA) measurements is a non-linear inversion problem. In this paper, a data-driven approach to SSL using TDOA measurements is considered. A neural network (NN) is viewed as an architecture constrained non-linear function, with its parameters learnt from the training data. We consider a three layer neural network with TDOA measurements between pairs of microphones as input features and source location in the Cartesian coordinate system as output. Experimentally, we show that, NN trained even on noise-less TDOA measurements can achieve good performance for noisy TDOA inputs also. These performances are better than the traditional spherical interpolation (SI) method. We show that the NN trained offline using simulated TDOA measurements, performs better than the SI method, on real-life speech signals in a simulated enclosure.                                                                                                                                                                                                                                                                                                                                                            |\n",
            "|8600014|Gaussian MAC with Feedback and Strictly Causal State Information                                            |\"Encoding\",\"Transmitters\",\"Decoding\",\"Indexes\",\"Additives\",\"Estimation error\",\"AWGN channels\"                                             |We consider a two user Gaussian multiple access channel with an additive Gaussian state process. The past values of both the state and the received symbols are strictly causally made available to the encoders at each instant. The capacity region for the noiseless case, without any feedback, was recently solved in literature. Here we study the model with noise as well as feedback. We propose a communication scheme which effectively utilizes the feedback symbols as well as the state information to enhance the achievable region. In particular, Wyner-Ziv binning on the state information and Ozarow feedback scheme for the MAC are effectively utilized, using a suitable interleaving technique. The obtained region is significantly better than the feedback capacity region with no state information.                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
            "|8600018|Efficient Detection of Phishing Attacks with Hybrid Neural Networks                                         |\"Phishing\",\"Convolution\",\"Deep learning\",\"Convolutional neural networks\",\"Uniform resource locators\",\"Computer architecture\"              |Many machine learning techniques and social engineering methods have been adopted and devised to combat phishing threats. In this paper, a novel hybrid deep learning model is proposed to identify phishing attacks. It incorporates two components: an autoencoder (AE) and a convolutional neural network (CNN). The AE is adopted to reconstruct features that enhances correlation relationship among the features explicitly. The results from the experiments show that the model is able to detect phishing attacks with a mean accuracy over 97.68%, yet it has high generalization ability and can detect phishing attacks in the receivable time scale.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
            "|8600029|A Social Bots Detection Model Based on Deep Learning Algorithm                                              |\"Feature extraction\",\"Metadata\",\"Detection algorithms\",\"Measurement\",\"Deep learning\",\"Social network services\",\"Data mining\"              |With the development of the Internet, social bots are increasingly spreading on social platforms. Therefore, an effective detection algorithm is demanded to detect these social bot accounts that endanger social networks. In this paper, a social bots detection model based on deep learning algorithm (DeBD) is proposed. The model mainly includes three layers. The first layer is the joint content feature extraction layer, which focuses on the feature extraction of the tweets content and the relationship between them. The second layer is the tweet metadata temporal feature extraction layer, which regards the tweet metadata as temporal information and uses this temporal information as the input of the LSTM to extract the user social activity temporal feature. The third layer is the feature fusing layer, which fuses the extracted joint content features with the temporal features to detect social bots. To evaluate the effectiveness of the DeBD model, we conducted experiments on three different types of new social bot data sets from the real world and the experiment results also demonstrate the effectiveness of our proposed model.                                                                                                                    |\n",
            "|8600031|Energy-Efficient Network Coding Scheme for Two-Way Relay Visible Light Communications                       |\"Relays\",\"Network coding\",\"Maximum likelihood estimation\",\"Radio frequency\",\"Receivers\",\"Channel estimation\",\"Visible light communication\"|In this paper, we consider a two-way relay (TWR) visible light communication (VLC) system consisting of two users which communicate to each other with the help of a relay. To achieve efficient transmission, we introduce network coding (NC) into VLC system and we develop two energy-efficient NC-based strategies, namely straightforward network coding over finite-alphabet sets (SNCF) scheme and physical-layer network coding over finite-alphabet sets (PNCF) scheme, which need three time slots and two time slots respectively to achieve the communication. Simulation results indicate that our proposed SNCF scheme and PNCF scheme both can achieve great performance gains over the traditional four time-slot transmission scheme.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
            "|8600032|Radio Classify Generative Adversarial Networks: A Semi-supervised Method for Modulation Recognition         |\"Modulation\",\"Training\",\"Signal to noise ratio\",\"Gallium nitride\",\"Generative adversarial networks\",\"Generators\",\"Deep learning\"          |We introduce Generative Adversarial Network (GAN) into the radio machine learning domain for the task of modulation recognition by proposing a general, scalable, end-to-end framework named Radio Classify Generative Adversarial Networks (RCGANs). This method naively learns its features through self-optimization during an extensive data-driven GPU-based training process. Several experiments are taken on a synthetic radio frequency dataset, simulation results show that, compared with some renowned deep learning methods and classic machine learning methods, the proposed method achieves higher or equivalent classification accuracy, superior data utilization, and presents robustness against noises.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
            "|8600034|Secrecy Anti-jamming Game Learning in D2D Underlay Cellular Networks with an Active Eavesdropper            |\"Jamming\",\"Games\",\"Device-to-device communication\",\"Relays\",\"Cellular networks\",\"Physical layer security\",\"Interference\"                  |In this paper, we study the physical layer security and transmission reliability problem where there is an active eavesdropper (AE) in the D2D underlaying cellular networks. We formulate the cooperation between the cellular user equipment (CUE) and the D2D user equipment (DUE), the completion between legitimate users and the AE to be a secrecy anti-jamming game. In the proposed game framework, DUE launches the cooperative relaying or the friendly jamming mode to help CUE to improve its anti-eavesdropping and anti-jamming performance. CUE gives different-level rewards for the assistance of the DUE. And AE shifts its attacking modes between actively jamming and passively eavesdropping to maximize the destruction for the D2D underlaying cellular networks. Under the perfect information, we prove the existence of the pure-strategy equilibrium of the proposed game. Under the imperfect information, we analyze the existence of the mixed-strategy equilibrium of the proposed game and propose a distributed Q-Iearning-based algorithm to converge to a mixed-strategy equilibrium. Simulation results show that the proposed algorithm is convergent and verify that average utilities of legitimate users are improved by the cooperation between CUE and DUE.|\n",
            "+-------+------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check keyword_1\n",
        "human.filter(lower(col(\"keyword\")).like(f\"%{keyword_1}%\")).show(10, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iCpXGNzKXf5",
        "outputId": "e5b904da-5d2e-4cc8-bbef-a76620c09986"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|id     |title                                                                                                                                    |keyword                                                                                                                                                         |abstract                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
            "+-------+-----------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|8600091|Improved Epoch Extraction Using Variational Mode Decomposition Based Spectral Smoothing of Zero Frequency Filtered Emotive Speech Signals|\"Smoothing methods\",\"Estimation\",\"Speech processing\",\"Frequency estimation\",\"Time-frequency analysis\",\"Resonant frequency\"                                      |The objective of the present work is to improve the epoch extraction performance from emotive speech by proposing a post processing approach to the conventional zero frequency filtering (ZFF) method using variational mode decomposition (VMD) based spectral smoothing. Due to the fast uncontrolled variations of the pitch in emotive speech signals, the reliable estimation of epochs is always challenging. In the proposed method, the spectra of the short frames of zero frequency filtered signal (ZFFS) is subjected variational mode decomposition to get component spectra in five modes. A smoothed short time spectra is then obtained by excluding the spectra from the two higher VMD modes which essentially have the high spectral variations. The modified ZFFS is then reconstructed using the sinusoidal parameters corresponding to single dominant frequency present in the smoothed spectra using VMD by parameter interpolation based sinusoidal synthesis. The resulting re-synthesized ZFFS has reduced spurious zero crossings as compared to that obtained from the conventional ZFF method for emotive speech signals. The effectiveness of the proposed VMD based spectral post processing is confirmed from the improved epoch identification rate and epoch identification accuracy across all the emotive utterances (with 7 emotions) present in German emotion speech database having simultaneous speech and electroglottographic (EGG) signal recordings. The performance of the proposed method is found to be better or comparable with the other existing ZFF based post processing methods proposed for emotive speech signals in terms of the epoch identification accuracy with respect to the corresponding reference epochs estimated from EGG signals.|\n",
            "|8600131|Cell-Phone Identification from Recompressed Audio Recordings                                                                             |\"Mel frequency cepstral coefficient\",\"Feature extraction\",\"Audio recording\",\"Forensics\",\"Social network services\",\"Speech recognition\"                          |Many audio forensic applications would benefit from the ability to classify audio recordings, based on characteristics of the originating device, particularly in social media platforms where an enormous amount of data is posted every day. This paper utilizes passive signatures associated with the recording devices, as extracted from recorded audio itself, in the absence of any extrinsic security mechanism such as digital watermarking, to identify the source cell-phone of recorded audio. It uses device-specific information present in low as well as high-frequency regions of the recorded audio. On the only publicly available dataset in this field, MOBIPHONE, the proposed system gives a closed set accuracy of 97.2% which matches the state of art accuracy reported for this dataset. On audio recordings which have undergone double compression, as typically happens for a recording posted on social media, the proposed system outperforms the existing methods (4% improvement in average accuracy).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
            "|8600157|Throat Microphone Speech Enhancement via Progressive Learning of Spectral Mapping Based on LSTM-RNN                                      |\"Speech enhancement\",\"Training\",\"Bandwidth\",\"Logic gates\",\"Microphones\",\"Task analysis\",\"Recurrent neural networks\"                                             |In this paper, we propose a progressive spectral mapping learning algorithm for throat microphone (TM) speech enhancement. Unlike previous full-band spectra mapping algorithms, this algorithm divides the spectra mapping from TM speech to Air-conducted (AC) speech into two tasks, one is the voice conversion task, and the other is the artificial bandwidth extension task. Long short-term memory recurrent neural network (LSTM-RNN) is further deployed as the mapping model. Objective evaluation results show that the TM speech quality is improved when compared with conventional full-band spectra mapping framework and DNN-based mapping model.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
            "|8600190|Manner of Articulation based Split Lattices for Phoneme Recognition                                                                      |\"Lattices\",\"Decoding\",\"Hidden Markov models\",\"Speech recognition\",\"Mel frequency cepstral coefficient\",\"Feature extraction\"                                     |Phoneme lattices have been shown to be a good choice to encode in a compact way alternative decoding hypotheses from a speech recognition system. However the optimal phoneme sequence is produced by tracing all the phoneme identities in the lattice. This not only makes the search space of the decoder huge but also the final phoneme sequence may be prone to have false substitutions or insertion errors. In this paper, we introduce the split lattice structures that is generated by splitting the speech frames based on the manner of articulation. Spectral flatness measure (SFM) is exploited to detect the two broad manner of articulation sonorants and non-sonorants. The manner of sonorants includes broadly the vowels, the semivowels and the nasals whereas the fricatives, stop consonants and closures belong to non-sonorants. The conventional way of speech decoder produces one lattice for one test utterance. In our work, we split the speech frames into sonorants and non-sonorants based on SFM knowledge and generate split lattices. The split lattice generated are modified according to the manner of articulation in each split so as to remove the irrelevant phoneme identities in the lattice. For instance, the sonorant lattice is forced to exclude the non-sonorant phoneme identities and hence minimizing false substitutions or insertion errors. The proposed split lattice structure based on sonority detection decreased the phone error rates by nearly 0.9 % when evaluated on core TIMIT test corpus as compared to the conventional decoding involved in the state-of-the-art Deep Neural Networks (DNN).                                                                                                                                 |\n",
            "|8600267|Approaches to Codec Independent Speaker Identification in Voip Speech                                                                    |\"Codecs\",\"Speech coding\",\"Speech recognition\",\"Bit rate\",\"Training\",\"Mel frequency cepstral coefficient\",\"Data models\"                                          |The performance of automatic speaker identification (ASI) systems on Voice over Internet Protocol (VoIP) speech varies with the type of codec used in the VoIP communication. The type of codec used depends on the service provider of the user. Thus there is a need for the codec-independent ASI systems to identify the speaker. Three modeling approaches based on UBM-GMM framework and i-vector framework are proposed to identify the speaker independent of codec used. These frameworks are also evaluated for the mismatch conditions with respect to the codec used in training and testing. The proposed approaches are evaluated on VoIP speech from four codecs with different bit rates along with uncoded speech.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
            "|8600307|Robust Hierarchical Learning for Non-Negative Matrix Factorization With Outliers                                                         |\"Hidden Markov models\",\"Data models\",\"Linear programming\",\"Robustness\",\"Dictionaries\",\"Speech processing\",\"Convergence\"                                         |Desirable properties of extensions of non-negative matrix factorization (NMF) include robustness in the presence of noises and outliers, ease of implementation, the guarantee of convergence, operation in an automatic fashion that trades off the balance between data approximation and model simplicity well, and the capability to model the inherently sequential structure of time-series signals. The state-of-the-art methods typically have only a subset of these aforementioned properties and seldom simultaneously possess them all. In this paper, we propose a novel approach that provides all these desirable properties by extending the automatic relevance determination framework in NMF from Tan and Févotte. Starting from an objective function derived from the maximum a posterior estimation of a Bayesian model, we develop majorization-minimization algorithms that work effectively to determine the correct model order, regardless of the impact of noise and outliers. Subsequently, we give a rigorous convergence analysis of the proposed algorithms. Moreover, convolutive bases are also incorporated in the basic model so that it is able to capture the richness of temporal continuity. We perform experiments on both synthetic and real-world data sets to show the efficiency and robustness of our approach.                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
            "|8600841|Treatment pillow for relieving snoring symptoms based on snore recognition                                                               |\"Hardware\",\"Sleep apnea\",\"Filter banks\",\"Speech recognition\",\"Training\",\"Feature extraction\",\"Microphones\"                                                      |The main purpose of this paper is to build an embedded platform based on TMS320VC5509A processor, and finally realizes the recognition of snore and controls pillow change the height to relieve snoring symptoms. The whole embedded hardware platform collects the sounds through the microphone module, and completes data storage, data processing and data interaction through other peripherals. After a series of pre-processing operations, short-time energy and short-time zero crossing rate dual threshold detection is selected as endpoint recognition algorithm, MFCC(Mel Frequency Cepstral Coefficient) is selected as feature extraction and KNN(k-nearest neighbors algorithm) is selected as recognition algorithm. The experimental results show that this hardware system can run well and the design is reasonable. At the same time, it can also achieve a good accuracy in snore analysis and recognition.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
            "|8604332|Energy-efficient MFCC extraction architecture in mixed-signal domain for automatic speech recognition                                    |\"Mel frequency cepstral coefficient\",\"Feature extraction\",\"Computer architecture\",\"Frequency-domain analysis\",\"Energy efficiency\",\"Automatic speech recognition\"|This paper proposes a novel processing architecture to extract Mel-Frequency Cepstrum Coefficients (MFCC) for automatic speech recognition. Inspired by the human ear, the energy-efficient analog-domain information processing is adopted to replace the energy-intensive Fourier Transform in conventional digital-domain. Moreover, the proposed architecture extracts the acoustic features in the mixed-signal domain, which significantly reduces the cost of Analog-to-Digital Converter (ADC) and the computational complexity. We carry out the circuit-level simulation based on 180nm CMOS technology, which shows an energy consumption of 2.4 nJ/frame, and a processing speed of 45.79 μs/frame. The proposed architecture achieves 97.2% energy saving and about 6.4× speedup than state of the art. Speech recognition simulation reaches the classification accuracy of 99% using the proposed MFCC features.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
            "|8604460|Spectral Masking in MFCC Calculation for Noisy Speech                                                                                    |\"Mel frequency cepstral coefficient\",\"Transform coding\",\"Estimation\",\"Harmonic analysis\",\"Psychoacoustic models\",\"Speech recognition\",\"Standards\"               |To increase noise immunity of the MFCC (mel-frequency cepstral coefficients) widely used for voice signal parametrisation it is suggested to use a psychoacoustic model of frequency masking. Additionally, taking into account the formation mechanism of the formant regions in the voice signal spectrum it is suggested to influence the spectral samples corresponding to multiple harmonics of the fundamental tone. The modified algorithm is investigated on the basis of the single word recognition system adapted for MFCC voice signal parametrisation only. The positive effect of using proposed additional voice signal transformation in the parametrisation algorithm is shown.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
            "|8606825|Speech perception based on mapping speech to image by using convolution neural network                                                   |\"Speech recognition\",\"Convolution\",\"Training\",\"Spectrogram\",\"Biological neural networks\",\"Computational modeling\",\"Neurons\"                                     |Although speech perception has been studied for more than sixty years and a great deal about how the system works has been researched, there is still more to be discovered. Previous research considered speech perception in only one aspect of sound such as the speech recognition problem. Speech perception focuses on the process that operates to decode speech sounds no matter what words those sounds might comprise. In this paper, we introduce a new approach for the speech perception as learning the map between speech and other information that we receive from the surrounding environment vice other sensation. So the speech perception problem would be turn into the relationship learning. In this model, we propose using a convolution neural network to map speech signal to image.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
            "+-------+-----------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check keyword_2\n",
        "human.filter(lower(col(\"keyword\")).like(f\"%{keyword_2}%\")).show(10, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4ineqjkKaks",
        "outputId": "7ac92985-6fc2-48f2-e47e-04c8b678f278"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|id     |title                                                                                                 |keyword                                                                                                                                      |abstract                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
            "+-------+------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|8600036|Pedestrian Detection and Attribute Analysis Program Based on CNN                                      |\"Training\",\"Image color analysis\",\"Detectors\",\"Feature extraction\",\"Semantics\",\"Monitoring\",\"Computational modeling\"                         |In recent years, deep learning object detectors including Fast/Faster R-CNN, SSD, R-FCN and Mask R-CNN have shown significant performance for general object detection except for pedestrians. The Region Proposal Network (RPN) in Faster R-CNN works well yet lacks of adaptability. Therefore, we propose an adaptive real-time pedestrian detection and attribute identification scheme based on Caffe. The first contribution is the adaptive threshold adjustment (ATA) algorithm for intelligent monitoring, utilizing the pedestrian movement information to adjust the threshold. Moreover, to overcome the time-consuming defect, we analyze the influences of the number of layers, the size of convolution kernels and the number of feature maps to reduce redundant computation while maintaining satisfactory performance. By optimizing the neutral network structure, choosing model parameters and data augmentation, a stable and well-performed model with fast detection rate and high accuracy is obtained. Besides, pedestrian information can also be identified in our program, offering better service in security monitoring, intelligent robots and other fields. Extensive experimental results demonstrate that even in complex and athletic scenarios, our method can make an improvement in quality and speed over state-of-the-art.                                                                                                                                                                                                                                                                                    |\n",
            "|8600042|Large-Scale Image Geo- Tagging Using Affective Classification                                         |\"Feature extraction\",\"Image color analysis\",\"Image classification\",\"Psychology\",\"Training\",\"Art\",\"Affective computing\"                       |Images have always had a significant effect on their viewers at an emotional level by portraying so much in a single frame. These emotions have also been involved in human decision making. Machines can also be made emotionally intelligent using ‘Affective Computing’, giving them the ability of decision making by involving emotions. Emotional aspect of machine learning has been used in areas like E-Health and E-learning etc. In this paper, the emotional aspect of machines has been used to perform Geo-tagging of an image. The proposed solution concentrates on a hybrid approach towards Affective Image Classification where the Elements-of-Art based emotional features (EAEF) and Principles-of-Art based emotional features (PAEF) are combined. Firstly, experiments are performed on these two sets of features individually. Then, these two sets are combined to obtain a Hybrid feature vector and same experiments are performed on this feature vector. On comparison of results, it is indicated that the hybrid approach gives better accuracy then either individual approach. Images in this research work are downloaded from Yahoo Flickr Creative Commons 100 Million (YFCC100M) dataset which contains the co-ordinates of millions of images and are free to use.                                                                                                                                                                                                                                                                                                                                             |\n",
            "|8600068|Person Tracking and Frontal Face Capture with UAV                                                     |\"Face\",\"Feature extraction\",\"Drones\",\"Target tracking\",\"Face detection\",\"Image color analysis\"                                               |In this paper, we present a method using an unmanned aerial vehicle (UAV) to track the specified walking person and automatically capture a frontal photo of the target. The proposed method is composed by three parts: person detection and recognition, face detection and feature points localization, and vision based UAV control. In the person tracking part, we employ the deep neural network YOLOv3 for person detection and Locality-constrained Linear Coding (LLC) method to match the specified target person. In terms of frontal face perception, we use Multi-task Cascaded Convolutional Neural Networks (MTCNN) for face detection. Based on the vision information obtained from the two modules, the UAV can fly around the target person and obtain the target's frontal face image. The outdoor experiments based on a Parrot Bebop2 drone verify the effectiveness and practicability of our method.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
            "|8600125|Weakly Supervised Learning of Object-Part Attention Model for Fine-Grained Image Classification       |\"Image classification\",\"Convolutional neural networks\",\"Dogs\",\"Automobiles\",\"Training\",\"Visualization\",\"Testing\"                             |Fine-grained classification is challengeable due to the small inter-class variance and large intra-class distance between fine-grained categories. The key to solve this problem is to locate the discriminative part in the image. In this paper we propose a weakly supervised method, which only need image-level label for fine-grained classification. In our model, the convolutional neural network (CNN) can location the discriminative region through attention and automatically focus on subtler features by zooming the discriminative region and feeding it to the next CNN. A Squeeze and Excitation (SE) module is employed for channel-wise attention, and a spatial constrain loss is utilized to keep the diversity of located part. We conduct experiments on CUB-2011-200, Stanford Dogs and Stanford Cars datasets to evaluate the performance of our model. The experimental results demonstrate the effectiveness of the proposed method as compared other methods.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
            "|8600173|Accurate Specified-Pedestrian Tracking from Unmanned Aerial Vehicles                                  |\"Target tracking\",\"Feature extraction\",\"Unmanned aerial vehicles\",\"Cameras\",\"Image color analysis\",\"Image reconstruction\",\"Legged locomotion\"|Recently, accurate target tracking is widely used in the field of Unmanned Aerial Vehicles (UAV). In this paper, we focus on the application of detecting and following a walking pedestrian in real time from the moving platform with many interferences. We present a scheme that uses CNN model (YOLO-V2) to detect pedestrian and matches the walking pedestrian with a postprocessing and feature queue and Locality constrained Linear Coding algorithm. After that the ground station receives and analyses the video stream from the parrot and sends back commands to control the motion of UAV. At the beginning of the tracking process, the UAV is hovering when one pedestrian will be selected as the special target. Visual information is acquired only through a front camera without assistant sensors. A parrot Bebop 2 is adopted in the experiment, which is the basis for doing experiments outdoors and experimental result verify the effectiveness of our solution.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
            "|8600200|Exploiting Class Hierarchies for Large-Scale Scene Classification Using Hybrid Discriminative Approach|\"Feature extraction\",\"Visualization\",\"Databases\",\"Training\",\"Image resolution\",\"Support vector machines\",\"Semantics\"                         |Humans seamlessly perceive a massive amount of information while observing a scene. Though humans recognize real-world scenes easily and accurately but its not the same for computers due to scene images variability, ambiguity, and diverse illumination and scale conditions. Scene classification is a fundamental problem which provides contextual information to guide other processes, such as browsing, content-based image retrieval and object recognition. A baseline model based on traditional bag of words model is built to better evaluate the proposed solution. Model based on the idea of fine to coarse category mappings is proposed, whose information is combined with the fusion of feature descriptors resulting in a single feature representation. This additional information enhances performance by exploiting hierarchical relationship among the scene categories. Effectiveness of the proposed approach is validated using different evaluation metrics. Proposed model performs considerably better compared to the given baseline as well as several state-of-the-art methods. Proposed framework ensures appropriate balance between time and accuracy of the model                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
            "|8600306|A Two-Stage Attribute-Constraint Network for Video-Based Person Re-Identification                     |\"Feature extraction\",\"Task analysis\",\"Measurement\",\"Image color analysis\",\"Dynamics\",\"Learning systems\",\"Semantics\"                          |Person re-identification has gradually become a popular research topic in many fields such as security, criminal investigation, and video analysis. This paper aims to learn a discriminative and robust spatial–temporal representation for video-based person re-identification by a two-stage attribute-constraint network (TSAC-Net). The knowledge of pedestrian attributes can help re-identification tasks because it contains high-level information and is robust to visual variations. In this paper, we manually annotate three video-based person re-identification datasets with four static appearance attributes and one dynamic appearance attribute. Each attribute is regarded as a constraint that is added to the deep network. In the first stage of the TSAC-Net, we solve the re-identification problem as a classification issue and adopt a multi-attribute classification loss to train the CNN model. In the second stage, two LSTM networks are trained under the constraint of identities and dynamic appearance attributes. Therefore, the two-stage network provides a spatial–temporal feature extractor for pedestrians in video sequences. In the testing phase, a spatial–temporal representation can be obtained by inputting a sequence of images to the proposed TSAC-Net. We demonstrate the performance improvement gained with the use of attributes on several challenging person re-identification datasets (PRID2011, iLIDS-VID, MARS, and VIPeR). Moreover, the extensive experiments show that our approach achieves state-of-the-art results on three video-based benchmark datasets.                    |\n",
            "|8600316|Deep Learning for Reliable Mobile Edge Analytics in Intelligent Transportation Systems: An Overview   |\"Cloud computing\",\"Reliability\",\"Image edge detection\",\"Computer architecture\",\"Intelligent sensors\",\"Intelligent vehicles\"                  |Intelligent transportation systems (ITSs) will be a major component of tomorrow's smart cities. However, realizing the true potential of ITSs requires ultralow latency and reliable data analytics solutions that combine, in real time, a heterogeneous mix of data stemming from the ITS network and its environment. Such data analytics capabilities cannot be provided by conventional cloud-centric data processing techniques whose communication and computing latency can be high. Instead, edge-centric solutions that are tailored to the unique ITS environment must be developed. In this article, an edge analytics architecture for ITSs is introduced in which data is processed at the vehicle or roadside smart sensor level to overcome the ITS's latency and reliability challenges. With a higher capability of passengers' mobile devices and intravehicle processors, such a distributed edge computing architecture leverages deep-learning techniques for reliable mobile sensing in ITSs. In this context, the ITS mobile edge analytics challenges pertaining to heterogeneous data, autonomous control, vehicular platoon control, and cyberphysical security are investigated. Then, different deep-learning solutions for such challenges are revealed. The discussed deep-learning solutions enable ITS edge analytics by endowing the ITS devices with powerful computer vision and signal processing functions. Preliminary results show that the introduced edge analytics architecture, coupled with the power of deep-learning algorithms, provides a reliable, secure, and truly smart transportation environment.|\n",
            "|8600329|Texture Feature Extraction Methods: A Survey                                                          |\"Feature extraction\",\"Histograms\",\"Symmetric matrices\",\"Entropy\",\"Image segmentation\",\"Transforms\",\"Remote sensing\"                          |Texture analysis is used in a very broad range of fields and applications, from texture classification (e.g., for remote sensing) to segmentation (e.g., in biomedical imaging), passing through image synthesis or pattern recognition (e.g., for image inpainting). For each of these image processing procedures, first, it is necessary to extract—from raw images—meaningful features that describe the texture properties. Various feature extraction methods have been proposed in the last decades. Each of them has its advantages and limitations: performances of some of them are not modified by translation, rotation, affine, and perspective transform; others have a low computational complexity; others, again, are easy to implement; and so on. This paper provides a comprehensive survey of the texture feature extraction methods. The latter are categorized into seven classes: statistical approaches, structural approaches, transform-based approaches, model-based approaches, graph-based approaches, learning-based approaches, and entropy-based approaches. For each method in these seven classes, we present the concept, the advantages, and the drawbacks and give examples of application. This survey allows us to identify two classes of methods that, particularly, deserve attention in the future, as their performances seem interesting, but their thorough study is not performed yet.                                                                                                                                                                                                                  |\n",
            "|8600333|Action-Stage Emphasized Spatiotemporal VLAD for Video Action Recognition                              |\"Feature extraction\",\"Optical imaging\",\"Image coding\",\"Image segmentation\",\"Streaming media\",\"Image recognition\",\"Aggregates\"                |Despite outstanding performance in image recognition, convolutional neural networks (CNNs) do not yet achieve the same impressive results on action recognition in videos. This is partially due to the inability of CNN for modeling long-range temporal structures especially those involving individual action stages that are critical to human action recognition. In this paper, we propose a novel action-stage (ActionS) emphasized spatiotemporal vector of locally aggregated descriptors (ActionS-ST-VLAD) method to aggregate informative deep features across the entire video according to adaptive video feature segmentation and adaptive segment feature sampling (AVFS-ASFS). In our ActionS-ST-VLAD encoding approach, by using AVFS-ASFS, the keyframe features are chosen and the corresponding deep features are automatically split into segments with the features in each segment belonging to a temporally coherent ActionS. Then, based on the extracted keyframe feature in each segment, a flow-guided warping technique is introduced to detect and discard redundant feature maps, while the informative ones are aggregated by using our exploited similarity weight. Furthermore, we exploit an RGBF modality to capture motion salient regions in the RGB images corresponding to action activity. Extensive experiments are conducted on four public benchmarks-HMDB51, UCF101, Kinetics, and ActivityNet for evaluation. Results show that our method is able to effectively pool useful deep features spatiotemporally, leading to the state-of-the-art performance for video-based action recognition.            |\n",
            "+-------+------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check keyword_3\n",
        "human.filter(lower(col(\"keyword\")).like(f\"%{keyword_3}%\")).show(10, truncate=False)"
      ],
      "metadata": {
        "id": "Fs4pKS2PK4Mu",
        "outputId": "376838e3-9621-4c01-ea34-cb382645dc15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|id     |title                                                                                                                   |keyword                                                                                                                                                            |abstract                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
            "+-------+------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|8600401|HeadNet: An End-to-End Adaptive Relational Network for Head Detection                                                   |\"Object detection\",\"Face recognition\",\"Detectors\",\"Face\",\"Feature extraction\",\"Context modeling\"                                                                   |Head detection plays an important role in localizing and identifying persons from visual data. Most existing methods treat head detection as a specific form of object detection. Head detection is nontrivial due to the considerable difficulty in building the local and global information under conditions of unconstrained pose and orientation. To address these issues, this paper presents an effective adaptive relational network to capture context information, which is greatly helpful to suppress missed detection. We show that the fundamental contextual properties, such as the global shape priors from different heads and the local adjacent relationship between the head and shoulders, can be systematically quantified by visual operators. Specifically, we propose a two-step search algorithm to quantify the global intergroup conflict with adaptive scale, pose and viewpoint. Meanwhile, a structured feature module is introduced to capture the local relation of intraindividual stability. Finally, the global priors and local relation are integrated seamlessly into a single-stage head detector that is end-to-end trainable. An extensive ablation analysis demonstrates the effectiveness of our approach. We achieve state-of-the-art results on two challenging datasets, i.e., HollywoodHeads and Brainwash.                                                                                                                           |\n",
            "|8603045|Towards a Simplification of Medical Documents                                                                           |\"Natural language processing\",\"Classification algorithms\",\"Medical diagnostic imaging\",\"Text analysis\",\"Training\",\"Tagging\"                                        |In this paper, we propose a solution to simplify reading and understanding medical documents via the automatic demystification of complex medical terms found in web pages. The suggested approach detects those terms using a combination of NLP and heuristics. It computes the probability that a word is a medical and complex term through text processing and analysis. It then makes use of an ad hoc dictionary to simplify the complex terms. Finally, it exploits the Microsoft Bing cognitive API to retrieve images and videos related to the complex medical terms detected and presents them to the users so that they may have a better understanding of the document being read.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
            "|8603586|Ligature Recognition in Urdu Caption Text using Deep Convolutional Neural Networks                                      |\"Videos\",\"Text recognition\",\"Databases\",\"Feature extraction\",\"Optical character recognition software\",\"Character recognition\",\"Image segmentation\"                 |Textual content in videos contain rich information that can be exploited for semantic indexing and subsequent retrieval as well as development of video analytics solutions. The key modules in a textual content based video retrieval system include detection (localization) of text followed by its recognition, the later being the subject of our study. More specifically, this paper presents a caption text recognition system targeting Urdu text. The technique relies on a holistic approach using ligatures as units of recognition. Data driven feature extraction techniques are employed using a number of pre-trained deep convolution neural networks. The networks are used as feature extractors as well as fine-tuned on the ligature dataset under study and realized high ligature recognition rates.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
            "|8604033|Improving Urdu Recognition Using Character-Based Artistic Features of Nastalique Calligraphy                            |\"Character recognition\",\"Feature extraction\",\"Shape\",\"Image segmentation\",\"Writing\",\"Text recognition\",\"Microsoft Windows\"                                         |The state-of-the-art Urdu recognition approaches for Nastalique use features along with the sequence of characters’ labels for classification and recognition. In Arabic-like cursive script, the characters are joined together to form a ligature. The conventional methods process the connected stroke of ligatures as a sequence of characters. However, connected stroke of a ligature image has a sequence of pairs of characters and their joiners, instead of a sequence of characters. The character has a distinctive shape that clearly distinguishes it from other characters. The joiner preserves the connecting stroke shape of a character with the next character. In this paper, an implicit Urdu character recognition technique is presented for the Nastalique writing style that is based on recognition of characters and joiners. The detailed analysis of the Nastalique calligraphy is carried out to extract the artistic features of characters and their joiners. The presented technique is tested on Dataset-1 of 1446 ligature classes covering 3309762 ligature instances and 91129 unique Urdu words. In addition, the system is also tested on 1600 text lines of UPTI dataset called Dataset-2. The character recognition accuracies are 95.58% and 98.37% on Dataset-1 and Dataset-2, respectively. The results reveal that the system outperforms the state-of-the-art hidden Markov models and deep learning-based Urdu recognition techniques.|\n",
            "|8606831|FVI: An End-to-end Vietnamese Identification Card Detection and Recognition in Images                                   |\"Text recognition\",\"Feature extraction\",\"Optical character recognition software\",\"Image recognition\",\"Computational modeling\",\"Object detection\",\"Computer science\"|The neccesity of digitializing all old constrained forms such identification card or register book have become a critical issue due to the importance of practical applications for sales or financial services. To address this issue, we develop an End-to-end Identification Card Recognition system which allows us to quickly detect, recognize text and extract important information from the ID card. We not only present the modeling technique for efficient detection and recognition of texts but also the architecture design of FVI which is currently deployed in several organizations. We performed extensive evaluations of the designed system as the verification of our efficient system for a large-scale detecion and recognition of constrained forms.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
            "|8607161|Preventing Neural Network Model Exfiltration in Machine Learning Hardware Accelerators                                  |\"Hardware\",\"Computational modeling\",\"Training\",\"Machine learning\",\"Data models\",\"Neural networks\",\"Context modeling\"                                               |Machine learning (ML) models are often trained using private datasets that are very expensive to collect, or highly sensitive, using large amounts of computing power. The models are commonly exposed either through online APIs, or used in hardware devices deployed in the field or given to the end users. This provides an incentive for adversaries to steal these ML models as a proxy for gathering datasets. While API-based model exfiltration has been studied before, the theft and protection of machine learning models on hardware devices have not been explored as of now. In this work, we examine this important aspect of the design and deployment of ML models. We illustrate how an attacker may acquire either the model or the model architecture through memory probing, side-channels, or crafted input attacks, and propose (1) power-efficient obfuscation as an alternative to encryption, and (2) timing side-channel countermeasures.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
            "|8607560|Emerging Simulation and VR for Green Innovations: A Case Study on Promoting a Zero-carbon Emission Platform in Hong Kong|\"Green products\",\"Technological innovation\",\"Solid modeling\",\"Data models\",\"Computational modeling\",\"Biological system modeling\",\"Context modeling\"                |Researchers are interested in applying different technologies for social innovations. Previous articles that discussed the potential role of different tasks in social and green innovation context are commonly seen. However, there are limited articles on establishing process models for technological application in the social and green innovation context. The lack of a proper process may hinder the benefits of these applications to the general public. To address the said gap for guiding the practitioners, we have chosen a case of applying VR and simulation for promoting a green deck project in Hong Kong. With references to our findings, we have established a 4-step process model for applying simulation and VR for green innovations, including planning, modelling, verifying and application.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
            "|8607908|The Sphere of Discourse and Text                                                                                        |\"Production\",\"Linguistics\",\"Semantics\",\"Natural language processing\",\"Computational linguistics\",\"Speech\",\"Text analysis\"                                          |This chapter addresses the key concepts in the domain of discourse analysis, and reviews the terminology. Discourse analysis focuses on coherence. Discourse analysis pertains to all extra‐sentential phenomena including those observed in texts. The discourse is made of utterances, sentences used in a specific context with a given communicative goal, which have the property of being coherent. The interpretation of an utterance does not only occur through the elements that are explicitly present. This chapter discusses the construction of a tree structure for discourse. This level of analysis can easily be considered the syntax of the discourse. Naturally, every syntactic structure needs a semantic framework for its interpretation and that is where Discourse Representation Theory (DRT) comes in. In modern semantics, a new movement emerged. The focus of this movement is discourse, which it considers to be the unit that must have a truth value, rather than the sentence.                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
            "|8609600|LASAGNE: Locality and Structure Aware Graph Node Embedding                                                              |\"Approximation algorithms\",\"Task analysis\",\"Training\",\"Machine learning algorithms\",\"Learning systems\",\"Context modeling\",\"Graph theory\"                           |In this work we propose LASAGNE, a methodology to learn locality and structure aware graph node embeddings in an unsupervised way. In particular, we show that the performance of existing random-walk based approaches depends strongly on the structural properties of the graph, e.g., the size of the graph, whether the graph has a flat or upward-sloping Network Community Profile (NCP), whether the graph is expander-like, whether the classes of interest are more k-core-like or more peripheral, etc. For larger graphs with flat NCPs that are strongly expander-like, existing methods lead to random walks that expand rapidly, touching many dissimilar nodes, thereby leading to lower-quality vector representations that are less useful for downstream tasks. Rather than relying on global random walks or neighbors within fixed hop distances, LASAGNE exploits strongly local Approximate Personalized PageRank stationary distributions to more precisely engineer local information into node embeddings. This leads, in particular, to more meaningful and more useful vector representations of nodes in poorly-structured graphs. We show that LASAGNE leads to significant improvement in downstream multi-label classification for larger graphs with flat NCPs and that it is comparable for smaller graphs with upward-sloping NCPs.                                                                                                                 |\n",
            "|8609619|Exploration of Word Embedding Model to Improve Context-Aware Recommender Systems                                        |\"Recommender systems\",\"Data mining\",\"Context modeling\",\"Feature extraction\",\"Proposals\",\"Data models\",\"Cleaning\"                                                   |Recommender systems aim to assist users by recommending items that may be of interest to them. Traditionally, these systems use only user and item information. Over time, new information is being used, such as contextual information, which has improved the accuracy of the generated recommendations. In this work, we propose a context-aware recommender method that extracts contextual information from textual reviews using a word embedding based model. In addition, we propose two ways of considering textual contexts in recommender systems, the \"Context of Reviews\" and the \"Context of Items\". We evaluated our proposal by using the Yelp dataset (RecSysChallenge 2013); three baselines; and four context-aware recommender systems. In general, our proposal seems to be superior to the three baselines, mainly considering the \"Context of Items\", and the results were promising, allowing some lines of future work.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
            "+-------+------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    }
  ]
}